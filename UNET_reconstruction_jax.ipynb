{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86365b-5593-4448-beac-b4807b4d2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload 0\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import importlib\n",
    "import jax\n",
    "import smoothing_library as SL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aca247a-bb16-4abc-81d3-28e3f5817d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting boxsize and  pixel grid, as well as width ofthe bin kF\n",
    "#where kf=k_min, as the lowest k represents the largest scale that fits in the box\n",
    "BoxSize = 1000.\n",
    "kF = 2*np.pi/BoxSize\n",
    "grid = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b419a",
   "metadata": {},
   "source": [
    "First start loading your halo fields at z=0 and the target fields which you would like to reconstruct, in our case this were z=127 dark matter denisty fields, of course from the same initial conditions/simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c51ec1-6cc4-4578-9f79-cc7720e020c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#load in the quijote simulation data from hb-CosmoGroup\n",
    "# X_data = np.array([np.load(f\"/scratch/hb-CosmoGroup/Quijote_Density_256/fiducial/0/df_m_256_PCS_fiducial_z=0_{i}.npy\") for i in tqdm(range(120))])\n",
    "# y_data = np.array([np.load(f\"/scratch/hb-CosmoGroup/Quijote_Density_256/fiducial/127/df_m_256_PCS_fiducial_z=127_{i}.npy\") for i in tqdm(range(3))])\n",
    "\n",
    "#load dark matter field, z=127, data from own scratch \n",
    "#y_data =  np.array([np.load(f\"/scratch/s3487202/Matter_Data/fiducial/0/df_m_256_PCS_fiducial_127_{i}.npy\") for i in tqdm(range(100))])\n",
    "\n",
    "\n",
    "#X_data_dm = np.array([np.load(f\"/scratch/hb-CosmoGroup/Quijote_Density_256/fiducial/0/df_m_256_PCS_fiducial_z=0_{i}.npy\") for i in tqdm(range(20))])\n",
    "# y_data = np.array([np.load(f\"/scratch/hb-CosmoGroup/Quijote_Density_256/fiducial/127/df_m_256_PCS_fiducial_z=127_{i}.npy\") for i in tqdm(range(6))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790430d5-cb44-49c1-9f5a-df49d1280391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb683c-0dc2-476a-b364-da493b16200f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load in variations of data, for onw purposes\n",
    "# X_data_halo =  np.array([np.load(f\"/scratch/s3487202/Halo_Data/fiducial/0/Not_Distorted/df_h_256_PCS_fiducial_0_nd_{i}.npy\") for i in tqdm(range(81))])\n",
    "# X_data_halo_t =  np.array([np.load(f\"/scratch/s3487202/Halo_Data/fiducial/0/threshold/df_h_256_PCS_fiducial_tresh_0_dx_{i}.npy\") for i in tqdm(range(81))])\n",
    "# # X_data_void =  np.array([np.load(f\"/scratch/s3487202/Void_Data/fiducial/0/Not_Distorted/df_v_256_fiducial_0_nd_{i}.npy\") for i in tqdm(range(80))])\n",
    "# # X_data_halo_void = np.array([np.load(f\"/scratch/s3487202/Halo_Data/fiducial/0/Halo_Void/df_h_v_256_fiducial_t_0_dz_{i}.npy\") for i in tqdm(range(80))])\n",
    "# #X_data_halo_s8_m =  np.array([np.load(f\"/scratch/s3487202/Halo_Data/s8_m/df_h_256_PCS_s8_m_0_dx_{i}.npy\") for i in tqdm(range(81))])\n",
    "# #X_data_h_v =  np.array([np.load(f\"/scratch/s3487202/Halo_Data/fiducial/0/Halo_Void/df_h_v_256_fiducial_0_dz_{i}.npy\") for i in tqdm(range(68))])\n",
    "\n",
    "# X_data_halo_t =np.squeeze(X_data_halo_t, axis=1)\n",
    "# X_data_halo = X_data_halo.astype(np.float32)\n",
    "# X_data_halo_t = X_data_halo_t.astype(np.float32)\n",
    "# X_data_halo_s8_m =np.squeeze(X_data_halo_s8_m, axis=1)\n",
    "# X_data_halo_z =  np.array([np.load(f\"/scratch/s3487202/Halo_Data/fiducial/0/0-1000/df_h_256_PCS_fiducial_0_dz_{i}.npy\") for i in tqdm(range(100))])\n",
    "\n",
    "# X_data_halo_z = np.squeeze(X_data_halo_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f1cf523-6408-449b-8be0-74e34cf3a073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #for mass thresshold, appendix of the published paper\n",
    "\n",
    "# import readfof\n",
    "# import readgadget\n",
    "# import redshift_space_library as RSL\n",
    "# import MAS_library as MASL\n",
    "# #load in halo data for different mass thresholds\n",
    "# #for fiducial\n",
    "# BoxSize = 1000.\n",
    "# grid = int(256)\n",
    "# Hubble = int(100)\n",
    "# redshift = 0\n",
    "\n",
    "\n",
    "# snapnum = 4\n",
    "# kF = 2*np.pi/BoxSize\n",
    "# threads=8\n",
    "# axis =2\n",
    "\n",
    "# numbers = list(range(380,400)) \n",
    "\n",
    "# # Pk_z0_array_z = np.zeros((len(numbers), Pks_32_z_s.shape[0], Pks_32_z_s.shape[1]))\n",
    "# # Pk_z127_rec_array_z = np.zeros((len(numbers), Pks_32_pred_z_s.shape[0], Pks_32_pred_z_s.shape[1]))\n",
    "# X_data_halo_z_quijote = np.zeros((len(numbers),256,256,256))\n",
    "# #X_data_halo_z_quijote_2_1 = np.zeros((len(numbers),256,256,256))\n",
    "# # X_data_halo_z_quijote_3_2 = np.zeros((len(numbers),256,256,256))\n",
    "# # print(X_data_halo_z_quijote_2_1.shape)\n",
    "\n",
    "# typey_list = ['fiducial']\n",
    "\n",
    "# for typey in typey_list:\n",
    "#     print(f'loop {typey} starts ')\n",
    "#     for num in numbers:\n",
    "#         print(num)    \n",
    "#         snapshot_number = num\n",
    "#         snapdir = f'/scratch/hb-CosmoGroup/Quijote_Halos/{typey}/{snapshot_number}' #folder hosting the catalogue\n",
    "       \n",
    "#         # read the halo catalogue\n",
    "    \n",
    "#         FoF = readfof.FoF_catalog(snapdir, snapnum, long_ids=False, swap=False, SFR=False, read_IDs=False)\n",
    "#         pos_h = FoF.GroupPos / 1e3\n",
    "#         vel_h = FoF.GroupVel*(1.0+redshift) #Halo peculiar velocities in km/s\n",
    "#         mass_h  = FoF.GroupMass*1e10 \n",
    "#         indices = np.where(mass_h >= 3.2e13)\n",
    "#         pos_h_filt = pos_h[indices]\n",
    "#         vel_h_filt = vel_h[indices]\n",
    "#         axis_list = [2] # z axis because void data is distorted in z axis\n",
    "#         delta_fields = []\n",
    "#         mean_fields = []\n",
    "#         # for axis in axis_list:\n",
    "#         RSL.pos_redshift_space(pos_h_filt, vel_h_filt, BoxSize, Hubble, redshift, axis)\n",
    "#         delta = np.zeros((grid, grid, grid), dtype=np.float32)\n",
    "#         #MASL.MA(pos_h_d, delta, BoxSize, MAS, verbose=verbose)\n",
    "#         MASL.MA(pos_h_filt, delta, BoxSize, 'PCS', verbose=False)\n",
    "#         # mean_fields.append(np.mean(delta, dtype=np.float32))\n",
    "#         delta /= np.mean(delta, dtype=np.float32)\n",
    "#         delta -= 1.0\n",
    "#         delta_z = delta\n",
    "#         X_data_halo_z_quijote[num-380] = delta_z\n",
    "#         #X_data_halo_z_quijote_2_1[num-380] = delta_z\n",
    "#         # X_data_halo_z_quijote_3_2[num-380] = delta_z\n",
    "\n",
    "# X_data_halo_z_quijote =  X_data_halo_z_quijote.astype(np.float32) \n",
    "# #X_data_halo_z_quijote_2_1 =  X_data_halo_z_quijote_2_1.astype(np.float32)\n",
    "# # X_data_halo_z_quijote_3_2 =  X_data_halo_z_quijote_3_2.astype(np.float32) \n",
    "# print('loop done')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52a5b90c-ab42-429a-9d57-cf4b6228933d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for mass index again for the appendix\n",
    "# import numpy as np\n",
    "# import readfof\n",
    "# import readgadget\n",
    "# import redshift_space_library as RSL\n",
    "# import MAS_library as MASL\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Parameters\n",
    "# BoxSize = 1000.0\n",
    "# grid = 256\n",
    "# Hubble = 100\n",
    "# redshift = 0\n",
    "# snapnum = 4\n",
    "# kF = 2 * np.pi / BoxSize\n",
    "# threads = 8\n",
    "# axis = 2\n",
    "# numbers = list(range(2))\n",
    "\n",
    "# # Initialize the data structure\n",
    "# X_data_halo_z_quijote_3_2 = np.zeros((len(numbers), grid, grid, grid), dtype=np.float32)\n",
    "\n",
    "# # Function to bin the mass data\n",
    "# def bin_mass_data(mass_data, bin_size):\n",
    "#     min_mass = np.min(mass_data)\n",
    "#     max_mass = np.max(mass_data)\n",
    "#     bins = np.arange(min_mass, max_mass + bin_size, bin_size)\n",
    "#     bin_indices = np.digitize(mass_data, bins) - 1\n",
    "#     print(bin_indices)\n",
    "#     print(len(bins))\n",
    "#     return bin_indices, bins\n",
    "\n",
    "# # Function to process the data and generate the halo fields\n",
    "# def process_halo_data(bin_size):\n",
    "#     for typey in ['fiducial']:\n",
    "#         print(f'Processing type: {typey} with bin size: {bin_size}')\n",
    "#         for idx, num in enumerate(numbers):\n",
    "#             print(f'Processing snapshot number: {num}')\n",
    "#             snapshot_number = num\n",
    "#             snapdir = f'/scratch/hb-CosmoGroup/Quijote_Halos/{typey}/{snapshot_number}'\n",
    "            \n",
    "#             # Read the halo catalogue\n",
    "#             FoF = readfof.FoF_catalog(snapdir, snapnum, long_ids=False, swap=False, SFR=False, read_IDs=False)\n",
    "#             pos_h = FoF.GroupPos / 1e3\n",
    "#             vel_h = FoF.GroupVel * (1.0 + redshift) # Halo peculiar velocities in km/s\n",
    "#             mass_h = FoF.GroupMass * 1e10\n",
    "\n",
    "#             # Bin the mass data\n",
    "#             bin_indices, bins = bin_mass_data(mass_h, bin_size)\n",
    "\n",
    "#             # Apply redshift space distortion\n",
    "#             RSL.pos_redshift_space(pos_h, vel_h, BoxSize, Hubble, redshift, axis)\n",
    "\n",
    "#             # Generate the halo field with weights based on mass bins\n",
    "#             delta = np.zeros((grid, grid, grid), dtype=np.float32)\n",
    "#             MASL.MA(pos_h, delta, BoxSize, 'PCS', W=bin_indices.astype(np.float32), verbose=False)\n",
    "#             delta /= np.mean(delta, dtype=np.float32)\n",
    "#             delta -= 1.0\n",
    "\n",
    "#             # Store the result\n",
    "#             X_data_halo_z_quijote_3_2[idx] = delta\n",
    "\n",
    "#     return X_data_halo_z_quijote_3_2\n",
    "\n",
    "# # Example usage\n",
    "# bin_size = 1e13  # Adjust the bin size as needed\n",
    "# halo_fields = process_halo_data(bin_size)\n",
    "\n",
    "# # Investigate distribution function\n",
    "# def investigate_distribution(data):\n",
    "#     data_array = data.flatten()\n",
    "#     print(\"Mean:\", np.mean(data_array))\n",
    "#     print(\"Median:\", np.median(data_array))\n",
    "#     print(\"Standard Deviation:\", np.std(data_array))\n",
    "    \n",
    "#     num_counts, bin_edges = np.histogram(data_array, bins=100)\n",
    "#     plt.hist(data_array, bins=50, color='blue', edgecolor='black')\n",
    "#     plt.xlim(0, 1e15)\n",
    "#     plt.title('Distribution of Values')\n",
    "#     plt.xlabel('Values')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()\n",
    "\n",
    "#     return num_counts, bin_edges\n",
    "\n",
    "\n",
    "# num_counts, bin_edges = investigate_distribution(mass_h)\n",
    "# print(num_counts)\n",
    "# total_halos = num_counts.sum()\n",
    "# num_mean = num_counts / total_halos\n",
    "\n",
    "# bin_indices = np.digitize(mass_h, bin_edges)\n",
    "# # bin_indices = np.minimum(bin_indices, 100)\n",
    "\n",
    "# data_point_bin_counts = (1 / num_mean[bin_indices - 1]) - 1\n",
    "# print(data_point_bin_counts)\n",
    "# print(bin_indices)\n",
    "# print(mass_h)\n",
    "\n",
    "# # Construct 3D halo field\n",
    "# delta_index = np.zeros((grid, grid, grid), dtype=np.float32)\n",
    "# delta_mass = np.zeros((grid, grid, grid), dtype=np.float32)\n",
    "\n",
    "# bin_indices = bin_indices.astype(np.float32)\n",
    "# MASL.MA(pos_h, delta_index, BoxSize, 'PCS', W=bin_indices, verbose=False)\n",
    "# MASL.MA(pos_h, delta_mass, BoxSize, 'PCS', W=mass_h, verbose=False)\n",
    "\n",
    "# print('%.3f < delta < %.3f' % (np.min(delta_mass), np.max(delta_mass)))\n",
    "# print('<delta> = %.3f' % np.mean(delta_mass))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c42e90",
   "metadata": {},
   "source": [
    "Make the 3D dark matter halo fields based on the positions and mass from the simulations. Set num bins to 0 for not doing any mass weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f60c5-cd00-4a49-a245-c79a0dd21d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import readfof\n",
    "import readgadget\n",
    "import redshift_space_library as RSL\n",
    "import MAS_library as MASL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "BoxSize = 1000.0\n",
    "grid = 256\n",
    "Hubble = 100\n",
    "redshift = 0\n",
    "snapnum = 4\n",
    "kF = 2 * np.pi / BoxSize\n",
    "threads = 8\n",
    "axis = 2 #axis you want to distort the fields 2 0,1,2 is x,y,z\n",
    "numbers = list(range(20))\n",
    "\n",
    "# Initialize the data structure\n",
    "\n",
    "\n",
    "# Function to bin the mass data based on the number of bins if you do not want to bin mass set to \n",
    "def bin_mass_data_equal_number(mass_data, num_bins):\n",
    "    # print(mass_data)\n",
    "    sorted_mass = np.sort(mass_data)\n",
    "    # print(sorted_mass)\n",
    "    bin_edges = np.interp(np.linspace(0, len(sorted_mass), num_bins + 1), np.arange(len(sorted_mass)), sorted_mass)\n",
    "    bin_indices = np.digitize(mass_data, bin_edges) \n",
    "    # print(bin_indices)\n",
    "    # print(bin_edges)\n",
    "    # print(len(bin_edges))\n",
    "    return bin_indices, bin_edges\n",
    "\n",
    "# Function to process the data and generate the halo fields\n",
    "def process_halo_data(num_bins):\n",
    "    for typey in ['fiducial']:\n",
    "        X_data_halo_z_quijote_3_2 = np.zeros((len(numbers), grid, grid, grid), dtype=np.float32)\n",
    "        print(f'Processing type: {typey} with {num_bins} bins')\n",
    "        for idx, num in enumerate(numbers):\n",
    "            print(f'Processing snapshot number: {num}')\n",
    "            snapshot_number = num\n",
    "            snapdir = f'/scratch/hb-CosmoGroup/Quijote_Halos/{typey}/{snapshot_number}'\n",
    "            \n",
    "            # Read the halo catalogue\n",
    "            FoF = readfof.FoF_catalog(snapdir, snapnum, long_ids=False, swap=False, SFR=False, read_IDs=False)\n",
    "            pos_h = FoF.GroupPos / 1e3\n",
    "            vel_h = FoF.GroupVel * (1.0 + redshift) # Halo peculiar velocities in km/s\n",
    "            mass_h = FoF.GroupMass * 1e10\n",
    "\n",
    "            # Bin the mass data\n",
    "            bin_indices, bin_edges = bin_mass_data_equal_number(mass_h, num_bins)\n",
    "            # Apply redshift space distortion\n",
    "            RSL.pos_redshift_space(pos_h, vel_h, BoxSize, Hubble, redshift, axis)\n",
    "\n",
    "            # Generate the halo field with weights based on mass bins\n",
    "            delta = np.zeros((grid, grid, grid), dtype=np.float32)\n",
    "            MASL.MA(pos_h, delta, BoxSize, 'PCS', W=bin_indices.astype(np.float32), verbose=False)\n",
    "            # MASL.MA(pos_h, delta, BoxSize, 'PCS', W=mass_h, verbose=False)\n",
    "            delta /= np.mean(delta, dtype=np.float32)\n",
    "            delta -= 1.0\n",
    "\n",
    "            # Store the result\n",
    "            X_data_halo_z_quijote_3_2[idx] = delta\n",
    "\n",
    "    return X_data_halo_z_quijote_3_2\n",
    "\n",
    "# Example usage\n",
    "num_bins = 0  # Adjust the number of bins as needed\n",
    "halo_fields_not_binned = process_halo_data(num_bins)\n",
    "# Investigate distribution function\n",
    "# def investigate_distribution(data):\n",
    "#     data_array = data.flatten()\n",
    "#     print(\"Mean:\", np.mean(data_array))\n",
    "#     print(\"Median:\", np.median(data_array))\n",
    "#     print(\"Standard Deviation:\", np.std(data_array))\n",
    "    \n",
    "#     num_counts, bin_edges = np.histogram(data_array, bins=100)\n",
    "#     plt.hist(data_array, bins=50, color='blue', edgecolor='black')\n",
    "#     plt.xlim(0, 1e15)\n",
    "#     plt.title('Distribution of Values')\n",
    "#     plt.xlabel('Values')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()\n",
    "\n",
    "#     return num_counts, bin_edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a74d547a-3efd-483f-9505-3c1b2bbd4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(halo_fields[0,80:100,30,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57127d70",
   "metadata": {},
   "source": [
    "Utilize the isotropic in fourier space to speed up training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d8cf60a-0109-477d-b672-16681b4c76d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.numpy.fft import rfftn, irfftn\n",
    "\n",
    "def cutfield(delta, BoxSize, grid, maxk_kF):\n",
    "    # Cuts the density field at kmax = maxk_kF * kF \n",
    "    kF = 2 * jnp.pi / BoxSize\n",
    "    cell_size = BoxSize / grid\n",
    "\n",
    "    # Create the k-space grid\n",
    "    kx = 2 * jnp.pi * jnp.fft.fftfreq(grid, cell_size)\n",
    "    ky = 2 * jnp.pi * jnp.fft.fftfreq(grid, cell_size)\n",
    "    kz = 2 * jnp.pi * jnp.fft.rfftfreq(grid, cell_size)\n",
    "    kx, ky, kz = jnp.meshgrid(kx, ky, kz, indexing=\"ij\")\n",
    "    kgrid = jnp.sqrt(kx**2 + ky**2 + kz**2)\n",
    "\n",
    "    # Create a boolean mask for the cut-off\n",
    "    bools = (kgrid >= maxk_kF * kF)\n",
    "\n",
    "    # Apply Fourier Transform, filter and then apply Inverse Fourier Transform\n",
    "    c_fftgrid = rfftn(delta)\n",
    "    c_fftgrid = jnp.where(bools, 0.+0.j, c_fftgrid)\n",
    "    r_fftgrid = irfftn(c_fftgrid)\n",
    "\n",
    "    return r_fftgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504a7a2-8714-4932-a2a0-bd5daf88a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy.fft import rfftn, irfftn\n",
    "\n",
    "# Vectorize cutfield for batch processing\n",
    "batch_cutfield = vmap(cutfield, in_axes=(0, None, None, None))\n",
    "\n",
    "# Assuming y_data is a JAX array and BoxSize, grid are defined\n",
    "batch_size = 10\n",
    "\n",
    "# Process each batch and normalize it before moving to the next batch\n",
    "y_data_cut = np.zeros((y_data.shape[0],y_data.shape[1],y_data.shape[2],y_data.shape[3]),dtype=np.float32)\n",
    "for i in range(0, y_data.shape[0], batch_size):\n",
    "    batch = y_data[i:i + batch_size]\n",
    "    processed_batch = batch_cutfield(batch, BoxSize, grid, 82.5)\n",
    "\n",
    "    # Normalize the processed batch\n",
    "    processed_batch /= processed_batch.reshape(processed_batch.shape[0], -1).std()\n",
    "    processed_batch = np.array(processed_batch)\n",
    "\n",
    "    y_data_cut[i:i + batch_size] = processed_batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7d565-017e-4ccb-98c5-14c116851737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(fields_x_smoothed[0,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "plt.imshow(X_data_halo_z[0,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# plt.colorbar()\n",
    "# plt.title('z=0')\n",
    "plt.savefig('Plots/presentation_input.pdf',format='pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(y_data_cut[20,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(y_data_scaled[0,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(y_data_cut[0,:,:,:].mean(2), extent=[0,1000,0,1000])\n",
    "\n",
    "# plt.xticks([])  # Remove x-axis ticks and labels\n",
    "# plt.yticks([])  # Remove y-axis ticks and labels\n",
    "# plt.gca().set_aspect('equal', adjustable='box')\n",
    "# plt.colorbar()\n",
    "plt.savefig('Plots/presentation_target.pdf',format='pdf',bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# plt.imshow(halo_fields_mass[2,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "\n",
    "# plt.xticks([])  # Remove x-axis ticks and labels\n",
    "# plt.yticks([]) \n",
    "# plt.colorbar()\n",
    "# # plt.savefig('Plots/Daan_input.png',format='png',bbox_inches='tight', pad_inches=0)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# plt.imshow(halo_fields_binned_200[2,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "\n",
    "# plt.xticks([])  # Remove x-axis ticks and labels\n",
    "# plt.yticks([]) \n",
    "# plt.colorbar()\n",
    "# # plt.savefig('Plots/Daan_input.png',format='png',bbox_inches='tight', pad_inches=0)\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(X_data_halo_z_quijote_2_1[2,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "\n",
    "# plt.xticks([])  # Remove x-axis ticks and labels\n",
    "# plt.yticks([]) \n",
    "# plt.colorbar()\n",
    "# # plt.savefig('Plots/Daan_input.png',format='png',bbox_inches='tight', pad_inches=0)\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(halo_fields_binned_100[20,:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "\n",
    "# plt.xticks([])  # Remove x-axis ticks and labels\n",
    "# plt.yticks([]) \n",
    "# plt.colorbar()\n",
    "# # plt.savefig('Plots/Daan_input.png',format='png',bbox_inches='tight', pad_inches=0)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816856b",
   "metadata": {},
   "source": [
    "Make the train validation and test sets from the density fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17d0d5c2-75ef-4163-b470-4ad0cabf61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #single halo data\n",
    "# X_data_halo_x = X_data_halo_x.astype(np.float16)\n",
    "# y_data_cut = y_data_cut.astype(np.float16)\n",
    "\n",
    "X_train = X_data_halo_z[:50][:,:,:,:,np.newaxis]\n",
    "X_val = X_data_halo_z[50:80][:,:,:,:,np.newaxis]\n",
    "X_test = X_data_halo_z[80:100][:,:,:,:,np.newaxis]\n",
    "\n",
    "# X_train_check = X_data_halo_z[:250][:,:,:,:,np.newaxis]\n",
    "# X_val_check = X_data_halo_z[250:380][:,:,:,:,np.newaxis]\n",
    "# X_test_check = X_data_halo_z[380:][:,:,:,:,np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "# X_train = halo_fields_binned_100[:250][:,:,:,:,np.newaxis]\n",
    "# X_val = halo_fields_binned_100[250:380][:,:,:,:,np.newaxis]\n",
    "# X_test = halo_fields_binned_100[380:][:,:,:,:,np.newaxis]\n",
    "\n",
    "# X_train = halo_fields_mass[:250][:,:,:,:,np.newaxis]\n",
    "# X_val = halo_fields_mass[250:380][:,:,:,:,np.newaxis]\n",
    "# X_test = halo_fields_mass[380:][:,:,:,:,np.newaxis]\n",
    "\n",
    "# X_train = fields_x_smoothed[:70][:,:,:,:,np.newaxis]\n",
    "# X_val = fields_x_smoothed[70:85][:,:,:,:,np.newaxis]\n",
    "# X_test = fields_x_smoothed[85:][:,:,:,:,np.newaxis]\n",
    "\n",
    "# X_train_void = X_data_void[:45][:,:,:,:,np.newaxis]\n",
    "# X_val_void = X_data_void[45:55][:,:,:,:,np.newaxis]\n",
    "# X_test_void = X_data_void[55:][:,:,:,:,np.newaxis]\n",
    "\n",
    "y_train = y_data_cut[:50][:,:,:,:,np.newaxis]\n",
    "y_val = y_data_cut[50:80][:,:,:,:,np.newaxis]\n",
    "y_test = y_data_cut[80:100][:,:,:,:,np.newaxis]\n",
    "\n",
    "# # y_train = y_data_cut[:250][:,:,:,:,np.newaxis]\n",
    "# # y_val = y_data_cut[250:380][:,:,:,:,np.newaxis]\n",
    "# y_test = y_data_cut[:20][:,:,:,:,np.newaxis]\n",
    "\n",
    "# y_train = y_data_scaled[:60][:,:,:,:,np.newaxis]\n",
    "# y_val = y_data_scaled[60:65][:,:,:,:,np.newaxis]\n",
    "# y_test = y_data_scaled[65:][:,:,:,:,np.newaxis]\n",
    "\n",
    "\n",
    "# # #for plotting\n",
    "# X_test_1 = halo_fields_binned_1[380:400][:,:,:,:,np.newaxis]\n",
    "# X_test_99 = halo_fields_binned_100[380:400][:,:,:,:,np.newaxis]\n",
    "# X_test_9999 = halo_fields_binned_1000[380:400][:,:,:,:,np.newaxis]\n",
    "# y_test = y_data_cut[:20][:,:,:,:,np.newaxis]\n",
    "# X_test_1 = halo_fields_binned_1[:20][:,:,:,:,np.newaxis]\n",
    "# X_test_99 = halo_fields_binned_100[:20][:,:,:,:,np.newaxis]\n",
    "# X_test_9999 = halo_fields_binned_1000[:20][:,:,:,:,np.newaxis]\n",
    "\n",
    "# print(np.sum(X_test_1 >= -1))\n",
    "# print(np.sum(X_test >= -1))\n",
    "# print(y_test.shape)\n",
    "# print(X_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13c2d7",
   "metadata": {},
   "source": [
    "Initiate the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f50f2fd4-d449-461b-af62-b8fb4cd867be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state, checkpoints\n",
    "from flax import linen as nn\n",
    "from typing import Any\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from itertools import cycle\n",
    "import optax\n",
    "import joblib\n",
    "from jax import jit\n",
    "\n",
    "\n",
    "def chunks(lst, size):\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i:i + size]\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    # A simple extension of TrainState to also include batch statistics\n",
    "    batch_stats: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "222db6a6-c3ad-451e-a780-8a032ee9665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name : str,\n",
    "                 model_class : nn.Module,\n",
    "                 model_hparams : dict,\n",
    "                 optimizer_name : str,\n",
    "                 optimizer_hparams : dict,\n",
    "                 # optimzer : \n",
    "                 exmp_imgs : Any,\n",
    "                 seed=41):\n",
    "        \"\"\"\n",
    "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
    "\n",
    "        Inputs:\n",
    "            model_name - String of the class name, used for logging and saving\n",
    "            model_class - Class implementing the neural network\n",
    "            model_hparams - Hyperparameters of the model, used as input to model constructor\n",
    "            optimizer_name - String of the optimizer name, supporting ['sgd', 'adam', 'adamw']\n",
    "            optimizer_hparams - Hyperparameters of the optimizer, including learning rate as 'lr'\n",
    "            exmp_imgs - Example imgs, used as input to initialize the model\n",
    "            seed - Seed to use in the model initialization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model_class\n",
    "        self.model_hparams = model_hparams\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.optimizer_hparams = optimizer_hparams\n",
    "        self.optimizer = None\n",
    "        self.seed = seed\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = self.model_class(**self.model_hparams)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.abspath(os.path.join(CHECKPOINT_PATH, self.model_name))\n",
    "        self.logger = logging.getLogger(model_name)\n",
    "        # Create jitted training and eval functions\n",
    "        init_rng = jax.random.PRNGKey(self.seed)\n",
    "        _, self.dropout_key = jax.random.split(init_rng)\n",
    "        self.create_functions()\n",
    "        self.state = None \n",
    "        self.init_model(exmp_imgs)\n",
    "        # self.init_optimizer()\n",
    "        # Initialize model\n",
    "\n",
    "\n",
    "    # def create_functions(self):\n",
    "    #     # Function to calculate the MSE loss for a model\n",
    "    #     def calculate_loss(params, batch_stats, dropout_key, batch, training):\n",
    "    #         imgs, labels = batch\n",
    "    #         rngs = {'dropout': dropout_key} if training else {}\n",
    "    #         # Run model. During training, we need to update the BatchNorm statistics and apply dropout.\n",
    "    #         outs = self.model.apply({'params': params, 'batch_stats': batch_stats},\n",
    "    #                                 imgs, training=training,\n",
    "    #                                 rngs=rngs, mutable=['batch_stats'] if training else False)\n",
    "    #         predictions, new_model_state = outs if training else (outs, None)\n",
    "    #         # Use Mean Squared Error as loss function\n",
    "    #         loss = optax.squared_error(predictions, labels).mean()\n",
    "    #         return loss, new_model_state\n",
    "        \n",
    "    #     # Training function in order toalso implement validation data setto backpropogate\n",
    "    #     def train_step(state, train_batch, val_batch, dropout_key):\n",
    "    #         def loss_fn(params):\n",
    "    #             train_loss, new_model_state_train = calculate_loss(params, state.batch_stats, dropout_key, train_batch, training=True)\n",
    "    #             val_loss, new_model_state_val = calculate_loss(params, state.batch_stats, dropout_key, val_batch, training=False)\n",
    "    #             combined_loss = train_loss + val_loss \n",
    "    #             return combined_loss, (new_model_state_train, new_model_state_val)\n",
    "        \n",
    "    #         ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    #         loss, (new_model_state_train, new_model_state_val) = ret\n",
    "    #         state = state.apply_gradients(grads=grads, batch_stats=new_model_state_train['batch_stats'])\n",
    "    #         return state, loss\n",
    "\n",
    "\n",
    "            \n",
    "    #     # Eval function\n",
    "    #     def eval_step(state, batch):\n",
    "\n",
    "    #         # For evaluation, only the loss is relevant\n",
    "    #         loss, _ = calculate_loss(state.params, state.batch_stats, None, batch, training=False)\n",
    "    #         return loss\n",
    "    \n",
    "    #     self.train_step = jax.jit(train_step)\n",
    "    #     self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def create_functions(self):\n",
    "        # @jit\n",
    "        # def compute_weights(y_true): #from the 3 checkpoint\n",
    "        #     a1, a2, a3, a4, a5, a6 = 0.65, 0.3,0.44, 0.6, 3, 0.8\n",
    "        #     b = -2.31# Continuity constraint\n",
    "        #     weights = jnp.where(y_true <= -3, jnp.abs(a4*y_true)**a5+a6, #5.29\n",
    "        #               jnp.where(y_true <= -1.5, jnp.abs(a1*y_true),\n",
    "        #               jnp.where(y_true <= -0.5, jnp.abs(a2*y_true)+a3,\n",
    "        #               jnp.where(y_true <= 0.5, a2,\n",
    "        #               jnp.where(y_true <= 1.5, jnp.abs(a2*y_true)+a3,\n",
    "        #               jnp.where(y_true <= 3, jnp.abs(a1*y_true),\n",
    "        #               jnp.abs(a4*y_true)**a5+a6))))))\n",
    "        #     return weights\n",
    "        \n",
    "        # @jit\n",
    "        # def compute_weights(y_true):\n",
    "\n",
    "        #     a1, a2, a3, a4, a5, a6, a7,a8 = 0.6, 0.25, 0.50, 0.55, 3, -0.25, 2, -0.17 #change a3 and a6 to make continious\n",
    "        #     # a1, a2, a3, a4, a5, a6 = 0.6, 0.25, 0.5, 0.5, 3, 0.8\n",
    "\n",
    "        #     weights =   jnp.where(y_true < -3, jnp.abs(a4*y_true)**a5+a6, #5.29  \n",
    "        #                 # jnp.where(y_true <= -3, jnp.abs(a4*y_true)**a5+a6, #5.29\n",
    "        #                 jnp.where(y_true < -1.5, jnp.abs(a1*y_true)**a7+a8,\n",
    "        #                 jnp.where(y_true < -0.8, jnp.abs(a2*y_true)+a3, #next step maybe make 0.4 bigger as the seen from true prob distri\n",
    "        #                 jnp.where(y_true <= 0.8, a1,\n",
    "        #                 jnp.where(y_true <= 1.5, jnp.abs(a2*y_true)+a3,\n",
    "        #                 jnp.where(y_true <= 3, jnp.abs(a1*y_true)**a7+a8,\n",
    "        #                 jnp.abs(a4*y_true)**a5+a6))))))\n",
    "        #                   # jnp.abs(a4*y_true)**a5+a6))))))\n",
    "        #     return weights\n",
    "        \n",
    "        # def gaussian_pdf(x, mu, sigma):\n",
    "        #     return jnp.exp(-0.5 * jnp.square((x - mu) / sigma)) / (jnp.sqrt(2 * jnp.pi) * sigma)\n",
    "\n",
    "        # def compute_weights(y_true, mu=0, sigma=1, g=1):\n",
    "\n",
    "        #     gaussian = g * gaussian_pdf(y_true, mu, sigma)\n",
    "        #     weights = 1.0-gaussian\n",
    "        #     return weights\n",
    "        \n",
    "        # def compute_weights(y_true, mu=0, scale=5, g=0.5):\n",
    "\n",
    "        #     distance = jnp.abs(y_true - mu)\n",
    "        #     # Normalize and invert the weights so that values further from mu are penalized more\n",
    "        #     weights =  g+(distance / scale)\n",
    "        #     # Ensure that weights are non-negative\n",
    "        #     # weights = jnp.clip(weights, 0, 1)\n",
    "        #     return weights\n",
    "\n",
    "        # def msle_loss(y_true, y_pred):\n",
    "        #     return jnp.mean(jnp.square(jnp.log1p(y_true) - jnp.log1p(y_pred)))\n",
    "            \n",
    "        def calculate_loss(params, batch_stats, dropout_key, batch, training):\n",
    "            imgs, labels = batch\n",
    "            rngs = {'dropout': dropout_key} if training else {}\n",
    "            outs = self.model.apply({'params': params, 'batch_stats': batch_stats},\n",
    "                                    imgs, training=training, rngs=rngs, \n",
    "                                    mutable=['batch_stats'] if training else False)\n",
    "            predictions, new_model_state = outs if training else (outs, None)\n",
    "    \n",
    "            # Calculate weights based on the true density values\n",
    "            # weights = compute_weights(labels)\n",
    "            # absolute_difference = jnp.abs(predictions - labels)\n",
    "\n",
    "            # weighted_difference = weights * absolute_difference\n",
    "    \n",
    "            # Different loss functions\n",
    "            # loss = weighted_difference.mean()\n",
    "            # Use Optax's huber_loss function\n",
    "            # loss = optax.huber_loss(predictions, labels, delta=0.5).mean()\n",
    "            # mean squared logarithmic error\n",
    "            # loss = msle_loss(labels, predictions)\n",
    "            loss = optax.squared_error(predictions, labels).mean()\n",
    "            return loss, new_model_state\n",
    "            # Eval function\n",
    "\n",
    "        @jit    \n",
    "        def train_step(state, train_batch, val_batch, dropout_key):\n",
    "            def loss_fn(params):\n",
    "                train_loss, new_model_state_train = calculate_loss(params, state.batch_stats, dropout_key, train_batch, training=True)\n",
    "                val_loss, new_model_state_val = calculate_loss(params, state.batch_stats, dropout_key, val_batch, training=False)\n",
    "                combined_loss = train_loss + val_loss \n",
    "                return combined_loss, (new_model_state_train, new_model_state_val)\n",
    "        \n",
    "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            loss, (new_model_state_train, new_model_state_val) = ret\n",
    "            state = state.apply_gradients(grads=grads, batch_stats=new_model_state_train['batch_stats'])\n",
    "            return state, loss\n",
    "        @jit\n",
    "        def eval_step(state, batch):\n",
    "\n",
    "            # For evaluation, only the loss is relevant\n",
    "            loss, _ = calculate_loss(state.params, state.batch_stats, None, batch, training=False)\n",
    "            return loss\n",
    "            \n",
    "        self.train_step = jax.jit(train_step)\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    \n",
    "    def init_model(self, exmp_imgs):\n",
    "        \"\"\"\n",
    "        Initialize model parameters and batch statistics, and create training state.\n",
    "        This method should be called only if there is no pre-trained model.\n",
    "        \"\"\"\n",
    "        if self.state is None:\n",
    "            # Initialize model only if self.state is None\n",
    "            init_rng = jax.random.PRNGKey(self.seed)\n",
    "            # Split the RNG key for different components\n",
    "            init_key, dropout_key = jax.random.split(init_rng, 2)\n",
    "        \n",
    "            # Initialize model with the structured RNG keys and example images\n",
    "            variables = self.model.init({'params': init_key, \n",
    "                                         'batch_stats': init_key, \n",
    "                                         'dropout': dropout_key}, \n",
    "                                        exmp_imgs, \n",
    "                                        mutable=['params', 'batch_stats'])\n",
    "        \n",
    "            # Extract initialized parameters and batch statistics\n",
    "            self.init_params = variables['params']\n",
    "            self.init_batch_stats = variables['batch_stats']\n",
    "            self.state = None\n",
    "            \n",
    "    def init_optimizer(self, num_epochs=None, num_steps_per_epoch=None): # exponential moving average opzoeken en toevoegen\n",
    "        if self.optimizer is None:\n",
    "  \n",
    "            if self.optimizer_name.lower() == 'adam':\n",
    "                opt_class = optax.adam\n",
    "            elif self.optimizer_name.lower() == 'adamw':\n",
    "                opt_class = optax.adamw\n",
    "            elif self.optimizer_name.lower() == 'sgd':\n",
    "                opt_class = optax.sgd\n",
    "            else:\n",
    "                raise ValueError(f'Unknown optimizer \"{self.optimizer_name}\"')\n",
    "\n",
    "            lr_schedule = optax.piecewise_constant_schedule(\n",
    "                init_value=self.optimizer_hparams.pop('lr'),\n",
    "                boundaries_and_scales={int(num_steps_per_epoch*num_epochs*0.95): 1e-3}\n",
    "            )\n",
    "\n",
    "            transf = [optax.clip(1.0)]\n",
    "            if opt_class == optax.sgd and 'weight_decay' in self.optimizer_hparams:  # wd is integrated in adamw\n",
    "                transf.append(optax.add_decayed_weights(self.optimizer_hparams.pop('weight_decay')))\n",
    "            self.optimizer = optax.chain(\n",
    "                *transf,\n",
    "                opt_class(lr_schedule, **self.optimizer_hparams)\n",
    "            )\n",
    "\n",
    "            self.state = TrainState.create(\n",
    "                apply_fn=self.model.apply,\n",
    "                params=self.init_params,\n",
    "                batch_stats=self.init_batch_stats,\n",
    "                tx=self.optimizer\n",
    "            )\n",
    "\n",
    "    \n",
    "\n",
    "    def train_model(self, X_train, y_train, X_val, y_val, num_epochs=5):\n",
    "        num_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "        # Initialize the optimizer only if it hasn't been initialized\n",
    "        if self.optimizer is None:\n",
    "            self.init_optimizer(num_epochs, num_steps_per_epoch)\n",
    "    \n",
    "        best_eval = float('inf')\n",
    "        for epoch_idx in tqdm(range(1, num_epochs + 1)):\n",
    "            # Split the dropout key at the start of each epoch\n",
    "            self.dropout_key, _ = jax.random.split(self.dropout_key)\n",
    "            \n",
    "            self.train_epoch(X_train, y_train, X_val, y_val, epoch=epoch_idx, dropout_key=self.dropout_key)\n",
    "       \n",
    "            eval_loss = self.eval_model(X_val, y_val)\n",
    "            print(f\"Validation Loss for Epoch {epoch_idx}: {eval_loss:.4f}\")\n",
    "            if eval_loss <= best_eval:\n",
    "                best_eval = eval_loss\n",
    "                self.save_model(step=epoch_idx)\n",
    "                #self.logger.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def train_epoch(self, X_train, y_train, X_val, y_val, epoch, dropout_key):\n",
    "        metrics = defaultdict(list)\n",
    "        val_iter =  cycle(zip(chunks(X_val, BATCH_SIZE), chunks(y_val, BATCH_SIZE)))  # Create a cycle iterator for validation data\n",
    "  # Create a cycle iterator for validation data\n",
    "    \n",
    "        for i in range(0, len(X_train), BATCH_SIZE):\n",
    "            train_batch_x = X_train[i:i+BATCH_SIZE]\n",
    "            train_batch_y = y_train[i:i+BATCH_SIZE]\n",
    "            train_batch = (train_batch_x, train_batch_y)\n",
    "    \n",
    "            val_batch_x, val_batch_y = next(val_iter)  # Get a batch from validation data\n",
    "            val_batch = (val_batch_x, val_batch_y)\n",
    "    \n",
    "            self.state, loss = self.train_step(self.state, train_batch, val_batch, dropout_key)\n",
    "            # print(f\"Average Loss for Epoch {epoch}: {loss:.4f}\")\n",
    "            metrics['loss'].append(loss)\n",
    "  \n",
    "    def eval_model(self, X, y):\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        for i in range(0, len(X), BATCH_SIZE):\n",
    "            batch_x = X[i:i+BATCH_SIZE]\n",
    "            batch_y = y[i:i+BATCH_SIZE]\n",
    "            batch = (batch_x, batch_y)\n",
    "\n",
    "            loss = self.eval_step(self.state, batch)\n",
    "            total_loss += loss * len(batch_x)\n",
    "            count += len(batch_x)\n",
    "        \n",
    "        avg_loss = total_loss / count\n",
    "        return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        save_path = os.path.join(self.log_dir, f'model.joblib')\n",
    "        \n",
    "        state_to_save = {\n",
    "            'params': self.state.params,\n",
    "            'batch_stats': self.state.batch_stats,\n",
    "            'optimizer_state': self.state.opt_state  # Assuming you have the optimizer state in your TrainState\n",
    "        }\n",
    "        joblib.dump(state_to_save, save_path)\n",
    "        print(f\"Model saved at {save_path}\")\n",
    "    \n",
    "\n",
    "    def load_model(self,num_epochs = 5):\n",
    "        load_path = os.path.join(self.log_dir, 'model.joblib')\n",
    "        num_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "        if os.path.exists(load_path):\n",
    "            state_dict = joblib.load(load_path)\n",
    "    \n",
    "            if 'optimizer_state' in state_dict:\n",
    "                # Initialize a new optimizer and create a new state\n",
    "                self.init_optimizer(num_epochs, num_steps_per_epoch)\n",
    "                self.state = self.state.replace(\n",
    "                    apply_fn=self.model.apply,\n",
    "                    params=state_dict['params'],\n",
    "                    batch_stats=state_dict['batch_stats'],\n",
    "                    tx=self.optimizer\n",
    "                )\n",
    "        else:\n",
    "            print(f\"No saved model found at {load_path}. Initializing new model.\")\n",
    "            self.init_model(self.example_images)\n",
    "\n",
    "    \n",
    "    def checkpoint_exists(self):\n",
    "        # Use the same directory as in save_model and load_model\n",
    "        checkpoint_file = os.path.join(self.log_dir, 'model.joblib')\n",
    "        return os.path.isfile(checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d3d1f8",
   "metadata": {},
   "source": [
    "The same but now with Exponential Moving Average -> more generalized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ef7e98c-836c-4321-969a-d86ab60013b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule_EMA:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name : str,\n",
    "                 model_class : nn.Module,\n",
    "                 model_hparams : dict,\n",
    "                 optimizer_name : str,\n",
    "                 optimizer_hparams : dict,\n",
    "                 # optimzer : \n",
    "                 exmp_imgs : Any,\n",
    "                 seed=41):\n",
    "        \"\"\"\n",
    "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
    "\n",
    "        Inputs:\n",
    "            model_name - String of the class name, used for logging and saving\n",
    "            model_class - Class implementing the neural network\n",
    "            model_hparams - Hyperparameters of the model, used as input to model constructor\n",
    "            optimizer_name - String of the optimizer name, supporting ['sgd', 'adam', 'adamw']\n",
    "            optimizer_hparams - Hyperparameters of the optimizer, including learning rate as 'lr'\n",
    "            exmp_imgs - Example imgs, used as input to initialize the model\n",
    "            seed - Seed to use in the model initialization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model_class\n",
    "        self.model_hparams = model_hparams\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.optimizer_hparams = optimizer_hparams\n",
    "        self.optimizer = None\n",
    "        self.seed = seed\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = self.model_class(**self.model_hparams)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.abspath(os.path.join(CHECKPOINT_PATH, self.model_name))\n",
    "        self.logger = logging.getLogger(model_name)\n",
    "        # Create jitted training and eval functions\n",
    "        init_rng = jax.random.PRNGKey(self.seed)\n",
    "        _, self.dropout_key = jax.random.split(init_rng)\n",
    "        self.create_functions()\n",
    "        self.state = None\n",
    "        self.ema =None\n",
    "        self.ema_state = None\n",
    "        self.init_model(exmp_imgs)\n",
    "        # self.init_optimizer()\n",
    "        # Initialize model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_functions(self):\n",
    "            \n",
    "        def calculate_loss(params, batch_stats, dropout_key, batch, training):\n",
    "            imgs, labels = batch\n",
    "            rngs = {'dropout': dropout_key} if training else {}\n",
    "            outs = self.model.apply({'params': params, 'batch_stats': batch_stats},\n",
    "                                    imgs, training=training, rngs=rngs, \n",
    "                                    mutable=['batch_stats'] if training else False)\n",
    "            predictions, new_model_state = outs if training else (outs, None)\n",
    "\n",
    "            loss = optax.squared_error(predictions, labels).mean()\n",
    "            return loss, new_model_state\n",
    "            # Eval function\n",
    "\n",
    "        @jit    \n",
    "        def train_step(state, ema_state, train_batch, val_batch, dropout_key):\n",
    "            def loss_fn(params):\n",
    "                train_loss, new_model_state_train = calculate_loss(params, state.batch_stats, dropout_key, train_batch, training=True)\n",
    "                val_loss, new_model_state_val = calculate_loss(params, state.batch_stats, dropout_key, val_batch, training=False)\n",
    "                combined_loss = train_loss + val_loss \n",
    "                return combined_loss, (new_model_state_train, new_model_state_val)\n",
    "        \n",
    "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            loss, (new_model_state_train, new_model_state_val) = ret\n",
    "            state = state.apply_gradients(grads=grads, batch_stats=new_model_state_train['batch_stats'])\n",
    "            _, ema_state = self.ema.update(state.params, ema_state)\n",
    "    # Create a new state that includes EMA parameters but other parts of the original state\n",
    "            # ema_state_complete = state.replace(params=ema_params)  # Assuming state is a PyTree and using replace method\n",
    "        \n",
    "            return state, ema_state, loss\n",
    "\n",
    "            \n",
    "            return state, loss\n",
    "        @jit\n",
    "        def eval_step(state, batch):\n",
    "\n",
    "            # For evaluation, only the loss is relevant\n",
    "            loss, _ = calculate_loss(state.params, state.batch_stats, None, batch, training=False)\n",
    "            return loss\n",
    "            \n",
    "        self.train_step = jax.jit(train_step)\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    " \n",
    "    def init_model(self, exmp_imgs):\n",
    "        \"\"\"\n",
    "        Initialize model parameters and batch statistics, and create training state.\n",
    "        This method should be called only if there is no pre-trained model.\n",
    "        \"\"\"\n",
    "        if self.state is None:\n",
    "            # Initialize model only if self.state is None\n",
    "            init_rng = jax.random.PRNGKey(self.seed)\n",
    "            # Split the RNG key for different components\n",
    "            init_key, dropout_key = jax.random.split(init_rng, 2)\n",
    "        \n",
    "            # Initialize model with the structured RNG keys and example images\n",
    "            variables = self.model.init({'params': init_key, \n",
    "                                         'batch_stats': init_key, \n",
    "                                         'dropout': dropout_key}, \n",
    "                                        exmp_imgs, \n",
    "                                        mutable=['params', 'batch_stats'])\n",
    "        \n",
    "            # Extract initialized parameters and batch statistics\n",
    "            self.init_params = variables['params']\n",
    "            self.init_batch_stats = variables['batch_stats']\n",
    "            self.state = None\n",
    "            \n",
    "    def init_optimizer(self, num_epochs=None, num_steps_per_epoch=None): # exponential moving average opzoeken en toevoegen\n",
    "        if self.optimizer is None:\n",
    "  \n",
    "            if self.optimizer_name.lower() == 'adam':\n",
    "                opt_class = optax.adam\n",
    "            elif self.optimizer_name.lower() == 'adamw':\n",
    "                opt_class = optax.adamw\n",
    "            elif self.optimizer_name.lower() == 'sgd':\n",
    "                opt_class = optax.sgd\n",
    "            else:\n",
    "                raise ValueError(f'Unknown optimizer \"{self.optimizer_name}\"')\n",
    "\n",
    "            lr_schedule = optax.piecewise_constant_schedule(\n",
    "                init_value=self.optimizer_hparams.pop('lr'),\n",
    "                boundaries_and_scales={int(num_steps_per_epoch*num_epochs*0.95): 1e-3}\n",
    "            )\n",
    "\n",
    "            transf = [optax.clip(1.0)]\n",
    "            if opt_class == optax.sgd and 'weight_decay' in self.optimizer_hparams:  # wd is integrated in adamw\n",
    "                transf.append(optax.add_decayed_weights(self.optimizer_hparams.pop('weight_decay')))\n",
    "            self.optimizer = optax.chain(\n",
    "                *transf,\n",
    "                opt_class(lr_schedule, **self.optimizer_hparams)\n",
    "            )\n",
    "\n",
    "            self.state = TrainState.create(\n",
    "                apply_fn=self.model.apply,\n",
    "                params=self.init_params,\n",
    "                batch_stats=self.init_batch_stats,\n",
    "                tx=self.optimizer\n",
    "            )\n",
    "\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_val, y_val, num_epochs=5):\n",
    "        num_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "        # Initialize the optimizer only if it hasn't been initialized\n",
    "        if self.optimizer is None:\n",
    "            print('optimizer is initialized')\n",
    "            self.init_optimizer(num_epochs, num_steps_per_epoch)\n",
    "      \n",
    "\n",
    "        _, self.ema_state = optax.ema(0.,debias=False).update(self.state.params, self.ema_state)\n",
    "        \n",
    "        best_eval = float('inf')\n",
    "        best_eval_ema = float('inf')\n",
    "\n",
    "        for epoch_idx in tqdm(range(1, num_epochs + 1)):\n",
    "            # Split the dropout key at the start of each epoch\n",
    "            self.dropout_key, _ = jax.random.split(self.dropout_key)\n",
    "            \n",
    "            self.train_epoch(X_train, y_train, X_val, y_val, epoch=epoch_idx, dropout_key=self.dropout_key)\n",
    "       \n",
    "            eval_loss, eval_loss_ema = self.eval_model(X_val, y_val)\n",
    "            print(f\"Validation Loss for Epoch {epoch_idx}: {eval_loss:.4f}\")\n",
    "            print(f\"Validation Loss EMA for Epoch {epoch_idx}: {eval_loss_ema:.4f}\")\n",
    "            if eval_loss <= best_eval:\n",
    "                best_eval = eval_loss\n",
    "                self.save_model(step=epoch_idx)\n",
    "            \n",
    "            if eval_loss_ema <= best_eval_ema or epoch_idx == num_epochs:\n",
    "                best_eval_ema = eval_loss_ema\n",
    "                self.save_model_ema(step=epoch_idx)\n",
    "                #self.logger.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def train_epoch(self, X_train, y_train, X_val, y_val, epoch, dropout_key):\n",
    "        metrics = defaultdict(list)\n",
    "        val_iter =  cycle(zip(chunks(X_val, BATCH_SIZE), chunks(y_val, BATCH_SIZE)))  # Create a cycle iterator for validation data\n",
    "  # Create a cycle iterator for validation data\n",
    "    \n",
    "        for i in range(0, len(X_train), BATCH_SIZE):\n",
    "            train_batch_x = X_train[i:i+BATCH_SIZE]\n",
    "            train_batch_y = y_train[i:i+BATCH_SIZE]\n",
    "            train_batch = (train_batch_x, train_batch_y)\n",
    "    \n",
    "            val_batch_x, val_batch_y = next(val_iter)  # Get a batch from validation data\n",
    "            val_batch = (val_batch_x, val_batch_y)\n",
    "    \n",
    "            self.state, self.ema_state, loss = self.train_step(self.state,self.ema_state, train_batch, val_batch, dropout_key)\n",
    "            # print(f\"Average Loss for Epoch {epoch}: {loss:.4f}\")\n",
    "            metrics['loss'].append(loss)\n",
    "  \n",
    "    def eval_model(self, X, y):\n",
    "        total_loss = 0\n",
    "        total_loss_ema = 0\n",
    "        count = 0\n",
    "        count_ema = 0\n",
    "        for i in range(0, len(X), BATCH_SIZE):\n",
    "            batch_x = X[i:i+BATCH_SIZE]\n",
    "            batch_y = y[i:i+BATCH_SIZE]\n",
    "            batch = (batch_x, batch_y)\n",
    "\n",
    "            loss = self.eval_step(self.state, batch)\n",
    "            # loss_ema = self.eval_step(self., batch)\n",
    "\n",
    "            # loss_ema = self.eval_step(TrainState(apply_fn = self.model.apply, params=self.ema_state, batch_stats=self.state.batch_stats, tx = self.optimizer), batch)\n",
    "       \n",
    "            #ema_state_complete = TrainState(params=ema_params, batch_stats=self.state.batch_stats, tx=self.state.tx, apply_fn=self.model.apply, opt_state =self.state.opt_state, step =self.state.step)\n",
    "            \n",
    "            # ema_params, ema_state = self.ema.update(self.state.params, self.ema_state)\n",
    "            # ema_state_complete = TrainState(params=self.ema_state.ema, batch_stats=self.state.batch_stats, tx=self.optimizer, apply_fn=self.model.apply, opt_state =self.state.opt_state, step =self.state.step)\n",
    "            # ema_state_complete = TrainState(params=self.ema_state.ema, batch_stats=self.state.batch_stats, tx=self.state.tx, apply_fn=self.model.apply, opt_state =self.state.opt_state, step =0)\n",
    "            # ema_state_complete = self.state.replace(apply_fn=self.model.apply,params=self.ema_state.ema,batch_stats=self.state.batch_stats,tx=self.optimizer)\n",
    "            ema_state_complete = self.state.replace(params=self.ema_state.ema)\n",
    "            # ema_state_complete = self.state.replace(params=ema_params)\n",
    "            loss_ema = self.eval_step(ema_state_complete, batch)\n",
    "            \n",
    "            total_loss += loss * len(batch_x)\n",
    "            total_loss_ema += loss_ema * len(batch_x)\n",
    "            \n",
    "            count += len(batch_x)\n",
    "            count_ema += len(batch_x)\n",
    "        \n",
    "        avg_loss = total_loss / count\n",
    "        avg_loss_ema = total_loss_ema / count_ema\n",
    "        return avg_loss, avg_loss_ema\n",
    "\n",
    "\n",
    "\n",
    "    def save_model_ema(self, step=0):\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        save_path = os.path.join(self.log_dir, f'model_ema.joblib')\n",
    "        \n",
    "        state_to_save = {\n",
    "            'params': self.ema_state.ema,\n",
    "            'batch_stats': self.state.batch_stats,\n",
    "            'optimizer_state': self.state.opt_state  # Assuming you have the optimizer state in your TrainState\n",
    "        }\n",
    "        joblib.dump(state_to_save, save_path)\n",
    "        print(f\"Model saved at {save_path}\")\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        save_path = os.path.join(self.log_dir, f'model.joblib')\n",
    "        \n",
    "        state_to_save = {\n",
    "            'params': self.state.params,\n",
    "            'batch_stats': self.state.batch_stats,\n",
    "            'optimizer_state': self.state.opt_state  # Assuming you have the optimizer state in your TrainState\n",
    "        }\n",
    "        joblib.dump(state_to_save, save_path)\n",
    "        print(f\"Model saved at {save_path}\")\n",
    "    \n",
    "\n",
    "    def load_model(self,num_epochs = 5):\n",
    "        load_path = os.path.join(self.log_dir, 'model.joblib')\n",
    "        num_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "        if os.path.exists(load_path):\n",
    "            state_dict = joblib.load(load_path)\n",
    "    \n",
    "            if 'optimizer_state' in state_dict:\n",
    "                # print('yes')\n",
    "                # Initialize a new optimizer and create a new state\n",
    "                self.init_optimizer(num_epochs, num_steps_per_epoch)\n",
    "                self.state = self.state.replace(\n",
    "                    apply_fn=self.model.apply,\n",
    "                    params=state_dict['params'],\n",
    "                    batch_stats=state_dict['batch_stats'],\n",
    "                    tx=self.optimizer\n",
    "                )\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"No saved model found at {load_path}. Initializing new model.\")\n",
    "            self.init_model(self.example_images)\n",
    "        self.ema = optax.ema(decay=0.995, debias=False)\n",
    "        self.ema_state = self.ema.init(self.state.params)\n",
    "        \n",
    "    \n",
    "    def checkpoint_exists(self):\n",
    "        # Use the same directory as in save_model and load_model\n",
    "        checkpoint_file = os.path.join(self.log_dir, 'model.joblib')\n",
    "        return os.path.isfile(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "904cfa53-5861-4bfa-97e1-fedda7090c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model_class, model_name, model_hparams, optimizer_name, optimizer_hparams, exmp_imgs, \n",
    "                     X_train, y_train, X_val, y_val, X_test, y_test, num_epochs=5):\n",
    "    trainer = TrainerModule(model_name, model_class, model_hparams, optimizer_name, optimizer_hparams, exmp_imgs)\n",
    "\n",
    "    # Check for the existence of a checkpoint. If one exists, load it; otherwise, initialize a new model.\n",
    "    if trainer.checkpoint_exists():\n",
    "        print(\"Checkpoint found. Loading and continuing training...\")\n",
    "        trainer.load_model(num_epochs=num_epochs)\n",
    "    else:\n",
    "        print(\"No checkpoint found. Initializing a new model...\")\n",
    "        trainer.init_model(exmp_imgs)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train_model(X_train, y_train, X_val, y_val, num_epochs=num_epochs)\n",
    "    \n",
    "    # Evaluate the model on validation and test datasets\n",
    "    val_loss = trainer.eval_model(X_val, y_val)\n",
    "    test_loss = trainer.eval_model(X_test, y_test)\n",
    "\n",
    "    return trainer, {'val_loss': val_loss, 'test_loss': test_loss}\n",
    "\n",
    "def train_classifier_EMA(model_class, model_name, model_hparams, optimizer_name, optimizer_hparams, exmp_imgs, \n",
    "                     X_train, y_train, X_val, y_val, X_test, y_test, num_epochs=5):\n",
    "    trainer = TrainerModule_EMA(model_name, model_class, model_hparams, optimizer_name, optimizer_hparams, exmp_imgs)\n",
    "\n",
    "    # Check for the existence of a checkpoint. If one exists, load it; otherwise, initialize a new model.\n",
    "    if trainer.checkpoint_exists():\n",
    "        print(\"Checkpoint found. Loading and continuing training...\")\n",
    "        trainer.load_model(num_epochs=num_epochs)\n",
    "    else:\n",
    "        print(\"No checkpoint found. Initializing a new model...\")\n",
    "        trainer.init_model(exmp_imgs)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train_model(X_train, y_train, X_val, y_val, num_epochs=num_epochs)\n",
    "    \n",
    "    # Evaluate the model on validation and test datasets\n",
    "    val_loss = trainer.eval_model(X_val, y_val)\n",
    "    test_loss = trainer.eval_model(X_test, y_test)\n",
    "\n",
    "    return trainer, {'val_loss': val_loss, 'test_loss': test_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589a5e3",
   "metadata": {},
   "source": [
    "Train the model! Your intern model details are saved in the checkpoint path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7545e-1ecf-47d7-9308-fc0957ffbeb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import my_module\n",
    "from importlib import reload\n",
    "from UNET_jax_eff import UNET3D_jax_e\n",
    "\n",
    "import jax.numpy as jnp\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "import optax\n",
    "import pickle\n",
    "\n",
    "# model_instance = UNET3D_jax_e(image_size=256, BoxSize=1000, n_base_filters=9, depth=6, dropout_rate=0.25)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "CHECKPOINT_PATH = \"work_test\"\n",
    "# CHECKPOINT_PATH = \"huber_loss_delta_0.5\"\n",
    "# CHECKPOINT_PATH = \"gaussian_test_heavy_test\"\n",
    "print(jax.devices())\n",
    "#trainer, results = train_classifier(\n",
    "trainer, results = train_classifier(\n",
    "    model_class=UNET3D_jax_e,\n",
    "    model_name=\"UNET3D_jax_e\", model_hparams={\"image_size\": 256, \"BoxSize\": 1000, \"n_base_filters\": 9, \"depth\": 6, \"dropout_rate\": 0.3},\n",
    "    optimizer_name=\"adamw\",\n",
    "    optimizer_hparams={\"lr\": 1e-3, \"weight_decay\": 1e-4},\n",
    "    exmp_imgs=jnp.ones((1, 256, 256, 256, 1)),\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_val=X_val, y_val=y_val,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    num_epochs= 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eaf0cb-01b0-44b8-a365-4310c6053d7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ii = 0\n",
    "\n",
    "import pickle\n",
    "from UNET_jax_eff import UNET3D_jax_e\n",
    "\n",
    "import jax.numpy as jnp\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "import optax\n",
    "#plot this one against the low sim one\n",
    "# Function to predict using the model\n",
    "# CHECKPOINT_PATH_w = \"train_250_val_130_mass_full\"\n",
    "# CHECKPOINT_PATH_m = \"train_250_val_130_mass_binned_99\"\n",
    "CHECKPOINT_PATH = \"train_250_val_130\"\n",
    "# CHECKPOINT_PATH = \"gaussian_test_heavy_test\"\n",
    "# params_file = os.path.join(CHECKPOINT_PATH, \"UNET3D_jax_e/model.pkl\") \n",
    "params_file = os.path.join(CHECKPOINT_PATH, \"UNET3D_jax_e/model.joblib\")# Assuming the parameters are saved as \"model_0.pkl\"\n",
    "# params_file_w = os.path.join(CHECKPOINT_PATH_w, \"UNET3D_jax_e/model.joblib\")# Assuming the parameters are saved as \"model_0.pkl\"\n",
    "# params_file_m = os.path.join(CHECKPOINT_PATH_m, \"UNET3D_jax_e/model.joblib\")\n",
    "# Load the model parameters\n",
    "# with open(params_file, 'rb') as f:\n",
    "#     loaded_data = pickle.load(f)\n",
    "#     params = loaded_data['params']\n",
    "#     batch_stats = loaded_data['batch_stats']\n",
    "\n",
    "with open(params_file, 'rb') as f:\n",
    "    loaded_data = joblib.load(f)\n",
    "    params = loaded_data['params']\n",
    "    batch_stats = loaded_data['batch_stats']\n",
    "\n",
    "# with open(params_file_w, 'rb') as f:\n",
    "#     loaded_data_w = joblib.load(f)\n",
    "#     params_w = loaded_data_w['params']\n",
    "#     batch_stats_w = loaded_data_w['batch_stats']\n",
    "\n",
    "\n",
    "# with open(params_file_m, 'rb') as f:\n",
    "#     loaded_data_m = joblib.load(f)\n",
    "#     params_m = loaded_data_m['params']\n",
    "#     batch_stats_m = loaded_data_m['batch_stats']\n",
    "    \n",
    "def predict(model, params, batch_stats, X):\n",
    "    return model.apply({'params': params, 'batch_stats': batch_stats}, X, mutable=False, training =False)\n",
    "\n",
    "\n",
    "model = UNET3D_jax_e(image_size=256, BoxSize=1000, n_base_filters=9, depth=6, dropout_rate=0.3) # should be the same as in training\n",
    "# model_w = UNET3D_jax_e(image_size=256, BoxSize=1000, n_base_filters=9, depth=6, dropout_rate=0.3)\n",
    "# model_m = UNET3D_jax_e(image_size=256, BoxSize=1000, n_base_filters=9, depth=6, dropout_rate=0.3)\n",
    "# _, initial_params = model.init(jax.random.PRNGKey(0), [(BATCH_SIZE, 256, 256, 256, 1)])\n",
    "# model = UNET3D_jax_t.init(initial_params)\n",
    "\n",
    "# Predict with the model\n",
    "# Here 'ii' should be an index value. Make sure it's defined\n",
    "prediction = predict(model,params,batch_stats, X_train[0][np.newaxis, :, :, :, :])\n",
    "# prediction = predict(model,params,batch_stats, X_test_1[ii][np.newaxis, :, :, :, :])\n",
    "# prediction_w = predict(model_w,params_w,batch_stats_w, X_test_9999[ii][np.newaxis, :, :, :, :])\n",
    "# prediction_m = predict(model_m,params_m,batch_stats_m, X_test_99[ii][np.newaxis, :, :, :, :])\n",
    "# Convert JAX array to numpy if needed\n",
    "one_pred= np.array(prediction[0,:,:,:])\n",
    "# one_pred_w = np.array(prediction_w[0,:,:,:])\n",
    "# one_pred_m = np.array(prediction_m[0,:,:,:])\n",
    "# print(np.sum(one_pred+1))\n",
    "# print(np.sum(X_test[ii]+1))\n",
    "# print(np.sum(y_test[ii]+1))\n",
    "\n",
    "# print(np.sum(y_test[ii]+1)-np.sum(one_pred+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d5e59",
   "metadata": {},
   "source": [
    "From here test your test set results to validate accuracy and generalized model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b29945-1347-4031-991c-60f9d7af4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 16,        # Default text size\n",
    "    'axes.labelsize': 18,   # Axis labels\n",
    "    'axes.titlesize': 20,   # Plot titles\n",
    "    'xtick.labelsize': 16,  # X-axis tick labels\n",
    "    'ytick.labelsize': 16   # Y-axis tick labels\n",
    "})\n",
    "\n",
    "def calculate_and_plot_probability_distribution(density_array, num_bins):\n",
    "    \"\"\"\n",
    "    Calculate and plot the probability distribution of density values in an array.\n",
    "\n",
    "    Args:\n",
    "    - density_array (numpy.ndarray): The array of density values.\n",
    "    - num_bins (int): The number of bins to use for the histogram.\n",
    "\n",
    "    Returns:\n",
    "    - bin_edges (numpy.ndarray): The edges of the bins.\n",
    "    - probabilities (numpy.ndarray): The probability for each bin.\n",
    "    \"\"\"\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(density_array, bins=num_bins, density=False)\n",
    "\n",
    "    # Normalize the counts to get probabilities\n",
    "    total_counts = np.sum(counts)\n",
    "    probabilities = counts / total_counts\n",
    "\n",
    "    # Plotting\n",
    "    plt.bar(bin_edges[:-1], probabilities, width=np.diff(bin_edges), edgecolor=\"black\", align=\"edge\")\n",
    "    plt.xlabel('Density')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Probability Distribution of Density')\n",
    "    # plt.ylim(0,0.002)\n",
    "    plt.show()\n",
    "\n",
    "    return bin_edges, probabilities\n",
    "\n",
    "# Example usage\n",
    "softgreen = (0.2, 0.7, 0.2)  # Lighter green\n",
    "softred = (0.9, 0.3, 0.3)    # More red, less pink  # Lighter red\n",
    "num_bins = 100  # Choose an appropriate number of bins\n",
    "# bin_edges, pred_prob = calculate_and_plot_probability_distribution(X_test[0], num_bins)\n",
    "bin_edges_1, pred_prob = calculate_and_plot_probability_distribution(one_pred, num_bins)\n",
    "# bin_edges, probabilities = calculate_and_plot_probability_distribution(one_pred_w, num_bins)\n",
    "bin_edges_2, true_prob = calculate_and_plot_probability_distribution(y_test[0], num_bins)\n",
    "# bin_edges, true_prob = calculate_and_plot_probability_distribution(y_data[0], num_bins)\n",
    "\n",
    "print(np.sum(pred_prob))\n",
    "print(np.sum(true_prob))\n",
    "\n",
    "print(np.sum(len(bin_edges_1)))\n",
    "print(np.sum(len(bin_edges_2)))\n",
    "\n",
    "num_bins_1 = 125\n",
    "num_bins_2 = 50\n",
    "# Calculate histograms for both arrays\n",
    "\n",
    "# Calculate histograms for both arrays\n",
    "counts1, bin_edges1 = np.histogram(one_pred, bins=num_bins_2, density=False)\n",
    "counts2, bin_edges2 = np.histogram(y_test[0], bins=num_bins_1, density=False)\n",
    "\n",
    "# Normalize counts to get probabilities\n",
    "probabilities1 = counts1 / np.sum(counts1)\n",
    "probabilities2 = counts2 / np.sum(counts2)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 5), sharey=True)\n",
    "\n",
    "# First subplot for the first dataset\n",
    "axs[0].bar(bin_edges2[:-1], probabilities2, width=np.diff(bin_edges2),color = softgreen, edgecolor=None, align='edge')\n",
    "axs[0].set_xlabel('Density')\n",
    "axs[0].set_ylabel('Probability')\n",
    "axs[0].set_title('Initial (z=127)')\n",
    "\n",
    "# Second subplot for the second dataset\n",
    "axs[1].bar(bin_edges1[:-1], probabilities1, width=np.diff(bin_edges1),color = softred, edgecolor = None, align='edge')\n",
    "axs[1].set_xlabel('Density')\n",
    "axs[1].set_title('Reconstruction')\n",
    "\n",
    "# # Set a common y-label for both subplots\n",
    "# fig.supylabel('Probability')\n",
    "# fig.suptitle('Probability Distributions of Two Datasets')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig('Plots/prob_distributions.png', format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0f176-6029-4329-b6b8-ec00dbdc2af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import Pk_library as PKL\n",
    "from matplotlib.pyplot import figure, show, rcParams\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.ticker import FuncFormatter, AutoMinorLocator, MultipleLocator\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,        # Default text size\n",
    "    'axes.labelsize': 20,   # Axis labels\n",
    "    'axes.titlesize': 22,   # Plot titles\n",
    "    'xtick.labelsize': 18,  # X-axis tick labels\n",
    "    'ytick.labelsize': 18   # Y-axis tick labels\n",
    "})\n",
    "\n",
    "\n",
    "# plt.imshow(X_test[ii,:,:,:,0].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Input: $z=0$\")\n",
    "# # plt.savefig(\"results/2D_z0_real.pdf\",format='pdf',bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(y_test[ii,:,:,:,0].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# # plt.colorbar()\n",
    "# plt.title(\"Target: $z=127$\")\n",
    "# # plt.savefig(\"Plots/2D_z127_rsd.pdf\",format='pdf',bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(one_pred_w[:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# # plt.imshow(one_pred_w[:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# # plt.colorbar()\n",
    "# plt.title(\"Output: reconstruction\")\n",
    "# # plt.savefig(\"Plots/2D_recon_real.pdf\",format='pdf',bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(one_pred[:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# # plt.imshow(one_pred_w[:,:].mean(2),extent=[0,1000,0,1000])\n",
    "# plt.xlabel('Mpc/h')\n",
    "# plt.ylabel('Mpc/h')\n",
    "# # plt.colorbar()\n",
    "# plt.title(\"Output: reconstruction\")\n",
    "# # plt.savefig(\"Plots/2D_recon_real.pdf\",format='pdf',bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(one_pred[:,:,:].mean(2),extent=[0,1000,0,1000])\n",
    "\n",
    "# plt.xticks([])  # Remove x-axis ticks and labels\n",
    "# plt.yticks([]) \n",
    "# plt.savefig('Plots/Daan_recon.png',format='png',bbox_inches='tight', pad_inches=0)\n",
    "# plt.show()\n",
    "\n",
    "# # plt.hist(X_test[ii].ravel(),bins=1000,density=True) #check why this has more bins\n",
    "# plt.hist(one_pred.ravel(),bins=100,density=True)\n",
    "# plt.hist(y_test[ii].ravel(),bins=100,density=True)\n",
    "# plt.title(\"Pixel Histogram\")\n",
    "# plt.xlim(-3,10)\n",
    "# plt.legend([\"z=0\",\"z=127 Prediction\",\"z=127 Truth\"])\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(X_test[ii,20,20,:,0])\n",
    "# plt.plot(one_pred[20,20])\n",
    "\n",
    "Pkcross_recon = PKL.XPk([one_pred[:,:,:,0],y_test[ii,:,:,:,0]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "cross_recon = Pkcross_recon.XPk[:,0,0]/np.sqrt(Pkcross_recon.Pk[:,0,0]*Pkcross_recon.Pk[:,0,1])\n",
    "# plt.plot(Pkcross_recon.k3D,cross_recon,label=\"reconstruction\")\n",
    "# plt.plot(Pkcross_recon.k3D,cross_recon,label=\"$\\\\bar{n}$ = $4.0e\\\\text{-}4$\")\n",
    "# plt.plot(Pkcross_recon.k3D,cross_recon,label=\"$M_{binning}$ = $1$\")\n",
    "\n",
    "# Pkcross_recon_m2 = PKL.XPk([one_pred_m[:,:,:,0],y_test[ii,:,:,:,0]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "# cross_recon_m2 = Pkcross_recon_m2.XPk[:,0,0]/np.sqrt(Pkcross_recon_m2.Pk[:,0,0]*Pkcross_recon_m2.Pk[:,0,1])\n",
    "# plt.plot(Pkcross_recon_m2.k3D,cross_recon_m2,label=\"$M_{binning}$ = $100$\")\n",
    "\n",
    "# plt.plot(Pkcross_recon_m2.k3D,cross_recon_m2,label=\"$\\\\bar{n}$ = $2.49e\\\\text{-}4$\")\n",
    "\n",
    "# Pkcross_recon_v = PKL.XPk([one_pred_w[:,:,:,0],y_test[ii,:,:,:,0]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "# cross_recon_v = Pkcross_recon_v.XPk[:,0,0]/np.sqrt(Pkcross_recon_v.Pk[:,0,0]*Pkcross_recon_v.Pk[:,0,1])\n",
    "# plt.plot(Pkcross_recon_v.k3D,cross_recon_v,label=\"$M_{binning}$ = full\")\n",
    "# plt.plot(Pkcross_recon_v.k3D,cross_recon_v,label=\"$\\\\bar{n}$ = $1.55e\\\\text{-}4$\")\n",
    "\n",
    "\n",
    "\n",
    "# Pkcross_recon_c = PKL.XPk([one_pred[:,:,:,0],one_pred_w[:,:,:,0]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "# cross_recon_c = Pkcross_recon_c.XPk[:,0,0]/np.sqrt(Pkcross_recon_c.Pk[:,0,0]*Pkcross_recon_c.Pk[:,0,1])\n",
    "# plt.plot(Pkcross_recon_c.k3D,cross_recon_c,label=\"$N_{\\\\rm train} = cross$\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(0, 81.25 * kF)\n",
    "plt.legend(fontsize=16, loc=3)\n",
    "plt.xlabel(\"$k$ [h/Mpc]\")\n",
    "plt.ylabel(\"$C_{X,Y}(k)$\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(which='major', linestyle='--', linewidth='0.7', color='black')\n",
    "plt.grid(which='minor', linestyle='--', linewidth='0.05', color='gray')\n",
    "\n",
    "ax = plt.gca()  # Get the current axes\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "# plt.savefig('Plots/mass_binning_cross_corr.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8e482-df68-4bdd-a9ee-528fa352e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "def concise_ticks(x, pos):\n",
    "    if x == 0:\n",
    "        return '0'\n",
    "    s = f\"{x:.2g}\"  # Adaptive formatting to two significant digits\n",
    "    if s.startswith('0'):\n",
    "        return s[1:]\n",
    "    elif s.startswith('-0'):\n",
    "        return '-' + s[2:]\n",
    "    else:\n",
    "        return s\n",
    "    \n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,        # Default text size\n",
    "    'axes.labelsize': 20,   # Axis labels\n",
    "    'axes.titlesize': 22,   # Plot titles\n",
    "    'xtick.labelsize': 18,  # X-axis tick labels\n",
    "    'ytick.labelsize': 18   # Y-axis tick labels\n",
    "})\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15.6, 5), sharey=True, constrained_layout=True)\n",
    "\n",
    "# First subplot\n",
    "# im1 = axs[0].imshow(X_test[0, 100:150, 100:150, 100:150].mean(2), extent=[390,585, 390, 585])\n",
    "im1 = axs[0].imshow(X_test[0, :, :, :].mean(0), extent=[0,1000, 0, 1000])\n",
    "axs[0].set_title(\"Final (z=0)\")\n",
    "axs[0].set_xlabel(\"Mpc/h\")\n",
    "axs[0].set_ylabel(\"Mpc/h\")\n",
    "# axs[0].xaxis.set_major_formatter(FuncFormatter(concise_ticks))\n",
    "# axs[0].yaxis.set_major_formatter(FuncFormatter(concise_ticks))\n",
    "# fig.colorbar(im1, ax=axs[0], orientation='vertical')\n",
    "\n",
    "divider1 = make_axes_locatable(axs[0])\n",
    "cax1 = divider1.append_axes(\"right\", size=\"4%\", pad=0.1)\n",
    "colorbar1 = fig.colorbar(im1, cax=cax1)\n",
    "colorbar1.formatter = FuncFormatter(concise_ticks)\n",
    "colorbar1.update_ticks()\n",
    "\n",
    "# Second subplot\n",
    "# im2 = axs[1].imshow(y_test[0, 100:150, 100:150, 100:150].mean(2), extent=[390, 585, 390, 585])\n",
    "im2 = axs[1].imshow(y_test[0, :, :, :].mean(0), extent=[0, 1000, 0, 1000])\n",
    "axs[1].set_title(\"Initial (z=127)\")\n",
    "axs[1].set_xlabel(\"Mpc/h\")\n",
    "# axs[1].xaxis.set_major_formatter(FuncFormatter(concise_ticks))\n",
    "# fig.colorbar(im2, ax=axs[1], orientation='vertical')\n",
    "\n",
    "divider2 = make_axes_locatable(axs[1])\n",
    "cax2 = divider2.append_axes(\"right\", size=\"4%\", pad=0.1)\n",
    "colorbar2 = fig.colorbar(im2, cax=cax2)\n",
    "colorbar2.formatter = FuncFormatter(concise_ticks)\n",
    "colorbar2.update_ticks()\n",
    "# axs[1].set_ylabel(\"Mpc/h\")\n",
    "# Third subplot\n",
    "# im3 = axs[2].imshow(one_pred[100:150, 100:150, 100:150].mean(2), extent=[390, 585, 390, 585])\n",
    "im3 = axs[2].imshow(one_pred[0:, 0:, :].mean(0), extent=[0, 1000, 0, 1000])\n",
    "axs[2].set_title(\"Reconstruction\")\n",
    "axs[2].set_xlabel(\"Mpc/h\")\n",
    "#fig.colorbar(ploty,cax=cbar_ax,ticks=[-1,-0.5,0,0.5,1],location='bottom') #for ticks specific\n",
    "# axs[2].xaxis.set_major_formatter(FuncFormatter(concise_ticks))\n",
    "# fig.colorbar(im3, ax=axs[2], orientation='vertical')\n",
    "# axs[2].set_ylabel(\"Mpc/h\")\n",
    "# Add a colorbar for all subplots\n",
    "# cbar = fig.colorbar(im3, ax=axs, location='right', shrink=0.94, aspect=30)\n",
    "\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "divider3 = make_axes_locatable(axs[2])\n",
    "cax3 = divider3.append_axes(\"right\", size=\"4%\", pad=0.1)\n",
    "colorbar3 = fig.colorbar(im3, cax=cax3)\n",
    "colorbar3.formatter = FuncFormatter(concise_ticks)\n",
    "colorbar3.update_ticks()\n",
    "\n",
    "# Add the colorbar to the new axis\n",
    "# fig.colorbar(im3, cax=cax)\n",
    "\n",
    "# fig.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "# Adjust spacing to remove white space between subplots\n",
    "# plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "# Show the figure\n",
    "plt.savefig('Plots/reconstruction_three_plot_dec.pdf',format='pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e56d1-2922-4511-aeed-b0cd6ceb96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure with 3 subplots\n",
    "\n",
    "# Plot for the first subplot\n",
    "\n",
    "ii = 0\n",
    "\n",
    "# K.clear_session()\n",
    "# model = UNET3D(256,BoxSize, n_base_filters=8,depth=5,loss_function=tf.keras.losses.mse,initial_learning_rate=1e-3)\n",
    "# model.load_weights(\"halo_Distorted_x\")\n",
    "# X_test_x = X_data_halo_x[65:][:,:,:,:,np.newaxis]\n",
    "\n",
    "# one_pred_x = model.predict(X_test_x[ii][np.newaxis,:,:,:])[0,:,:,:]\n",
    "\n",
    "\n",
    "im = axes[0].imshow(y_test[ii][:, :, :1].mean(2), extent=[0, 1000, 0, 1000])\n",
    "axes[0].set_xlabel('Mpc/h')\n",
    "axes[0].set_ylabel('Mpc/h')\n",
    "axes[0].set_title(\"Output: reconstruction\")\n",
    "fig.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Plot for the second subplot\n",
    "im = axes[1].imshow(one_pred[:, :, :1].mean(2), extent=[0, 1000, 0, 1000])\n",
    "axes[1].set_xlabel('Mpc/h')\n",
    "axes[1].set_ylabel('Mpc/h')\n",
    "axes[1].set_title(\"Output: reconstruction\")\n",
    "fig.colorbar(im, ax=axes[1])\n",
    "\n",
    "# Plot for the third subplot\n",
    "residual = y_test[ii] - one_pred\n",
    "im = axes[2].imshow(residual[:, :, :1].mean(2), extent=[0, 1000, 0, 1000])\n",
    "axes[2].set_xlabel('Mpc/h')\n",
    "axes[2].set_ylabel('Mpc/h')\n",
    "axes[2].set_title(\"Output: reconstruction\")\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "Pkcross = PKL.XPk([X_test[ii,:,:,:,0],y_test[ii,:,:,:,0]], 1000., axis=0, MAS=['PCS',\"PCS\"], threads=8)\n",
    "cross_pre = Pkcross.XPk[:,0,0]/np.sqrt(Pkcross.Pk[:,0,0]*Pkcross.Pk[:,0,1])\n",
    "plt.plot(Pkcross.k3D,cross_pre,label=\"final\")\n",
    "\n",
    "# Pkcross_recon_s = PKL.XPk([one_pred_s[:,:,:,0],y_test[ii,:,:,:,0]], 1000., axis=0, MAS=[None,\"PCS\"], threads=8)\n",
    "# cross_recon_s = Pkcross_recon_s.XPk[:,0,0]/np.sqrt(Pkcross_recon_s.Pk[:,0,0]*Pkcross_recon_s.Pk[:,0,1])\n",
    "\n",
    "Pkcross_recon = PKL.XPk([one_pred[:,:,:,0],y_test[ii,:,:,:,0]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "cross_recon = Pkcross_recon.XPk[:,0,0]/np.sqrt(Pkcross_recon.Pk[:,0,0]*Pkcross_recon.Pk[:,0,1])\n",
    "\n",
    "plt.plot(Pkcross_recon.k3D,cross_recon,label=\"$C_{\\\\rm residual,init}$\")\n",
    "# plt.plot(Pkcross_recon_s.k3D,cross_recon_s,label=\"$C_{\\\\rm second recon,init}$, $z=127-127$, $256^3$\")\n",
    "# plt.plot(Pkcross_recon_v.k3D,cross_recon_v,label=\"$C_{\\\\rm recon,init}$\")\n",
    "\n",
    "\n",
    "plt.xlim(0,100*kF)\n",
    "plt.legend(fontsize='medium', loc='lower left')\n",
    "plt.xlabel(\"$k$ [h/Mpc]\")\n",
    "plt.ylabel(\"$C_{X,Y}(k)$\")\n",
    "plt.ylim(0,1)\n",
    "plt.grid()\n",
    "# plt.savefig('Plots/sigma_8.pdf',format='pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811e7ab-2a84-48fc-8b5c-95c110dd8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, show, rcParams\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.ticker import FuncFormatter, AutoMinorLocator, MultipleLocator\n",
    "\n",
    "\n",
    "def to_percent(y, position):\n",
    "    # Convert the y-axis values to percentage string format without a decimal point for whole numbers\n",
    "    if y % 1 == 0:\n",
    "        s = f\"{int(y)}\"\n",
    "    else:\n",
    "        s = f\"{y:.1f}\"\n",
    "    return s + '%'\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,        # Default text size\n",
    "    'axes.labelsize': 22,   # Axis labels\n",
    "    'axes.titlesize': 22,   # Plot titles\n",
    "    'xtick.labelsize': 20,  # X-axis tick labels\n",
    "    'ytick.labelsize': 20,   # Y-axis tick labels\n",
    "    'font.weight': 'normal', # Set global font weight to normal\n",
    "    'axes.labelweight': 'normal', # Ensure normal weight for axis labels\n",
    "    'axes.titleweight': 'normal'  # Ensure normal weight for titles\n",
    "})\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "frame = fig.add_axes([0,0,1,1])\n",
    "\n",
    "frame.plot(Pkcross_recon.k3D,cross_recon,label=\"reconstruction\")\n",
    "frame.plot(Pkcross.k3D,cross_pre,label=\"final\")\n",
    "\n",
    "frame.set_xlim(0,81.25*kF)\n",
    "\n",
    "frame.set_ylim(0,1)\n",
    "\n",
    "frame.legend(loc = 'lower left', fontsize =16)\n",
    "# frame.set_title()\n",
    "\n",
    "frame.set_xlabel(\"$k$ [h/Mpc]\")\n",
    "\n",
    "frame.grid(which='major', linestyle='--', linewidth='0.6', color='black')\n",
    "frame.grid(which='minor', linestyle='-', linewidth='0.25', color='gray')\n",
    "frame.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "frame.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "plt.xlabel(\"$k$ [h/Mpc]\")\n",
    "plt.ylabel(\"$C_{X,Y}(k)$\")\n",
    "plt.savefig('Plots/compare_train.png',format='png',bbox_inches='tight')\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed953dc",
   "metadata": {},
   "source": [
    "Compute power spectra using the pylians documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98cad5-0aa7-4acb-aea9-2f23c2ff2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing power spectra using pylians doc for the monopole and quadrupole\n",
    "import Pk_library as PKL\n",
    "BoxSize = 1000. #Mpc/h\n",
    "kF = 2*np.pi/BoxSize #h/Mpc\n",
    "grid = 256\n",
    "threads=8\n",
    "axis =0\n",
    "#single example with data\n",
    "\n",
    "Pk = PKL.Pk(X_test[0][:,:,:,0], BoxSize, axis, 'PCS', threads, verbose =True)\n",
    "k       = Pk.k3D\n",
    "Pk0     = Pk.Pk[:,0] #monopole\n",
    "Pk2     = Pk.Pk[:,1] #quadrupole\n",
    "\n",
    "\n",
    "#single example with prediction\n",
    "pred = model.predict(X_test[0][np.newaxis,:,:,:], verbose=True)[0,:,:,:,0]\n",
    "Pk = PKL.Pk(pred, BoxSize, axis, 'PCS', threads, verbose= True)\n",
    "k       = Pk.k3D\n",
    "Pk0_pred     = Pk.Pk[:,0] #monopole\n",
    "Pk2_pred     = Pk.Pk[:,1] #quadrupole\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc911e",
   "metadata": {},
   "source": [
    "Use Densityfield3D module from Thomas Floss in order to calculate the spectra, can be done in batches for compuatation time splitted in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca059b07-d6d0-4f13-86bb-5197cb06a58d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from UNET import UNET3D\n",
    "from powerspec import PowSpec\n",
    "from DensityFieldTools import DensityField3D, PkX\n",
    "from DensityFieldToolsTensor import DensityField3DTF\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "BoxSize = 1000. #Mpc/h\n",
    "kF = 2*np.pi/BoxSize #h/Mpc\n",
    "grid = 256\n",
    "\n",
    "K.clear_session()\n",
    "model = UNET3D(256,BoxSize, n_base_filters=8,depth=5,loss_function=tf.keras.losses.mse,initial_learning_rate=1e-3)\n",
    "model.load_weights(\"halo_Distorted_x\")\n",
    "\n",
    "\n",
    "# single example\n",
    "pred = model.predict(X_data_halo_x[1][np.newaxis,:,:,:], verbose=True)[0,:,:,:,0]\n",
    "print(pred.shape)\n",
    "#single example after this only have to run the fields with the powspec\n",
    "powspec = PowSpec(256,1000.,'PCS')\n",
    "pk_tf = powspec.Pk(pred)\n",
    "\n",
    "# for multiple measurments of power spectra\n",
    "\n",
    "Pk_z0_array = np.zeros((500,  pk_tf.shape[0],pk_tf.shape[1],))\n",
    "Pk_z127_rec_array = np.zeros((500, pk_tf.shape[0],pk_tf.shape[1],))\n",
    "print(\"loop starts\")\n",
    "typey = 's8_p'\n",
    "#power spectra for the final z=0 data\n",
    "for i in range(500):\n",
    "    j = i\n",
    "    print(j) #keep track how for in the loop we are\n",
    "    X_data =  np.load(f\"/scratch/s3487202/Halo_Data/{typey}/df_h_256_PCS_{typey}_0_dx_{j}.npy\")\n",
    "    data_single = np.squeeze(X_data)\n",
    "    Pks = powspec.Pk(data_single)\n",
    "    Pk_z0_array[i]= Pks#[:,:][:81]\n",
    "    preds = np.squeeze(model.predict(data_single[np.newaxis,:,:,:,np.newaxis], verbose=False)[0,:,:,:])\n",
    "\n",
    "    Pks_pred = powspec.Pk(preds)\n",
    "    Pk_z127_rec_array[i] = Pks_pred#[:,:][:81]#*(0.011402733**2 / 0.01052**2)\n",
    "\n",
    "\n",
    "#saving the data\n",
    "\n",
    "print(Pk_z0_array.shape)\n",
    "print(Pk_z127_rec_array.shape)\n",
    "print(Pk_z127_rec_array[499])\n",
    "\n",
    "\n",
    "\n",
    "# np.save(f\"Results/Power_Spectra/halo_data_z0_{typey}_0-2999.npy\", Pk_z0_array)\n",
    "# np.save(f\"Results/Power_Spectra/halo_data_z127_{typey}_0-2999.npy\", Pk_z127_rec_array)\n",
    "\n",
    "np.save(f\"/scratch/s3487202/Results/power_spectrum/halo_data_z0_{typey}.npy\", Pk_z0_array)\n",
    "np.save(f\"/scratch/s3487202/Results/power_spectrum/halo_data_z127_{typey}.npy\", Pk_z127_rec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd09eec-2187-4bf8-9842-64b998be15f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#calcuation of the bispectra\n",
    "#single example\n",
    "from bispec import BiSpec\n",
    "from UNET import UNET3D\n",
    "\n",
    "BoxSize = 1000. #Mpc/h\n",
    "kF = 2*np.pi/BoxSize #h/Mpc\n",
    "grid = 256\n",
    "\n",
    "K.clear_session()\n",
    "model = UNET3D(256,BoxSize, n_base_filters=8,depth=5,loss_function=tf.keras.losses.mse,initial_learning_rate=1e-3)\n",
    "model.load_weights(\"halo_Distorted_x\")\n",
    "print(X_data_halo_x[1][np.newaxis,:,:,:].shape)\n",
    "pred =  np.squeeze(model.predict(X_data_halo_x[1][np.newaxis,:,:,:,np.newaxis], verbose=False)[0,:,:,:])\n",
    "print(pred.shape)\n",
    "bispec = BiSpec(256,1000.,3.,3.,27,None,verbose=False)\n",
    "bk_tf = bispec.Bk(pred, verbose = False)\n",
    "\n",
    "Bk_z0_array = np.zeros((500, bk_tf.shape[0],bk_tf.shape[1],))\n",
    "Bk_z127_rec_array =  np.zeros((500, bk_tf.shape[0],bk_tf.shape[1],))\n",
    "print(\"loop starts\")\n",
    "typey = 's8_p'\n",
    "\n",
    "for i in range(500):\n",
    "    j= i\n",
    "    print(j) #keep track how for in the loop we are\n",
    "    X_data =  np.load(f\"/scratch/s3487202/Halo_Data/{typey}/df_h_256_PCS_{typey}_0_dx_{j}.npy\")\n",
    "    data_single = np.squeeze(X_data)\n",
    "    Bks = bispec.Bk(data_single,verbose=False)\n",
    "    Bk_z0_array[i]= Bks#[:,:][:81]\n",
    "    preds = np.squeeze(model.predict(data_single[np.newaxis,:,:,:,np.newaxis], verbose=False)[0,:,:,:])\n",
    "    Bks = bispec.Bk(preds,verbose=False)\n",
    "    Bk_z127_rec_array[i] = Bks\n",
    "    \n",
    "print(Bk_z0_array.shape)\n",
    "print(Bk_z127_rec_array.shape)\n",
    "print(Bk_z127_rec_array[499])\n",
    "\n",
    "\n",
    "np.save(f\"/scratch/s3487202/Results/bispectrum/halo_data_z0_{typey}.npy\", Bk_z0_array)\n",
    "np.save(f\"/scratch/s3487202/Results/bispectrum/halo_data_z127_{typey}.npy\", Bk_z127_rec_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a750f2-5de2-4b6f-8369-2bdb1bd61bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bk_z127_rec_array[499])\n",
    "print(Bk_z127_rec_array[498])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c82e2-b3ef-4fa3-a3fc-8b311e389bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "typey = 'fiducial'\n",
    "array  = np.load(f\"/scratch/s3487202/Results/bispectrum/halo_data_z0_{typey}_3000-4499.npy\")\n",
    "array_2 = np.load(f\"/scratch/s3487202/Results/bispectrum/halo_data_z0_{typey}_4500-5999.npy\")\n",
    "\n",
    "array_comb = np.concetenate(array,array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560c13a-e9d1-4776-80b6-c3251daf1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "typey = 'fiducial'\n",
    "# array_z0 = np.load(f\"Results/Power_Spectra/halo_data_z0_{typey}_0-2999.npy\")\n",
    "# array_z127 = np.load(f\"Results/Power_Spectra/halo_data_z127_{typey}_0-2999.npy\")\n",
    "# print(array_z0.shape)\n",
    "np.save(f\"/scratch/s3487202/Results/bispectrum/halo_data_z0_{typey}_6000-8999.npy\", Bk_z0_array)\n",
    "np.save(f\"/scratch/s3487202/Results/bispectrum/halo_data_z127_{typey}_6000-8999.npy\", Bk_z127_rec_array)\n",
    "\n",
    "print(Bk_z127_rec_array[1499])\n",
    "print(Bk_z127_rec_array[1498])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc89b5",
   "metadata": {},
   "source": [
    "Some residual plotting for own investigation, not needed to save the spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25b627-9eb3-4903-82fd-05b59b31d7a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Pk_library as PKL\n",
    "kF = 2*np.pi/BoxSize\n",
    "numbers = list(range(81)) \n",
    "cross_pre_x_arr = np.zeros((len(numbers),221), dtype=np.float32)\n",
    "cross_pre_arr = np.zeros((len(numbers),221), dtype=np.float32)\n",
    "for i in numbers:\n",
    "\n",
    "    Pkcross_pre_x = PKL.XPk([X_data_halo_x[i],y_data_cut[i]], 1000., axis=0, MAS=['PCS',\"PCS\"], threads=8)\n",
    "    cross_pre_x = Pkcross_pre_x.XPk[:,0,0]/np.sqrt(Pkcross_pre_x.Pk[:,0,0]*Pkcross_pre_x.Pk[:,0,1])\n",
    "\n",
    "\n",
    "    Pkcross_pre = PKL.XPk([X_data_halo[i],y_data_cut[i]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "    cross_pre = Pkcross_pre.XPk[:,0,0]/np.sqrt(Pkcross_pre.Pk[:,0,0]*Pkcross_pre.Pk[:,0,1])\n",
    "    \n",
    "    cross_pre_x_arr[i] = cross_pre_x\n",
    "    cross_pre_arr[i] = cross_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47f1d0-a356-4ceb-9c76-0a2d811b6f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cross_recon_x_arr = np.zeros((len(numbers),221), dtype=np.float32)\n",
    "cross_recon_arr = np.zeros((len(numbers),221), dtype=np.float32)\n",
    "\n",
    "K.clear_session()\n",
    "model = UNET3D(256,BoxSize, n_base_filters=8,depth=5,loss_function=tf.keras.losses.mse,initial_learning_rate=1e-3)\n",
    "model.load_weights(\"halo_Distorted_x\")\n",
    "for i in numbers:\n",
    "\n",
    "    X_test_x = X_data_halo_x[i,:,:,:,np.newaxis]\n",
    "    y_test_x = y_data_cut[i,:,:,:,np.newaxis]\n",
    "    one_pred_x = model.predict(X_test_x[np.newaxis,:,:,:])[0,:,:,:]\n",
    "    Pkcross_recon_x = PKL.XPk([one_pred_x[:,:,:,0],y_test_x[:,:,:,0]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "    cross_recon_x = Pkcross_recon_x.XPk[:,0,0]/np.sqrt(Pkcross_recon_x.Pk[:,0,0]*Pkcross_recon_x.Pk[:,0,1])\n",
    "\n",
    "    cross_recon_x_arr[i] = cross_recon_x\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "model = UNET3D(256,BoxSize, n_base_filters=8,depth=5,loss_function=tf.keras.losses.mse,initial_learning_rate=1e-3)\n",
    "model.load_weights(\"halo_not_Distorted\")\n",
    "for i in numbers:  \n",
    "\n",
    "    X_test = X_data_halo[i,:,:,:,np.newaxis]\n",
    "    y_test = y_data_cut[i,:,:,:,np.newaxis]\n",
    "    one_pred = model.predict(X_test[np.newaxis,:,:,:])[0,:,:,:]\n",
    "    Pkcross_recon = PKL.XPk([one_pred[:,:,:,0],y_test[:,:,:,0]], 1000., axis=0, MAS=[\"PCS\",\"PCS\"], threads=8)\n",
    "    cross_recon = Pkcross_recon.XPk[:,0,0]/np.sqrt(Pkcross_recon.Pk[:,0,0]*Pkcross_recon.Pk[:,0,1])\n",
    "\n",
    "    cross_recon_arr[i] = cross_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75399db0-964d-479e-86a1-3d9ad00eda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, show, rcParams\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.ticker import FuncFormatter, AutoMinorLocator, MultipleLocator\n",
    "\n",
    "\n",
    "def to_percent(y, position):\n",
    "    # Convert the y-axis values to percentage string format without a decimal point for whole numbers\n",
    "    if y % 1 == 0:\n",
    "        s = f\"{int(y)}\"\n",
    "    else:\n",
    "        s = f\"{y:.1f}\"\n",
    "    return s + '%'\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (5, 5),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large',\n",
    "         'font.weight': 'bold'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "difference = np.mean((cross_pre_arr - cross_pre_x_arr)/cross_pre_x_arr,axis = 0)*100\n",
    "difference_recon = np.mean((cross_recon_arr - cross_recon_x_arr)/cross_recon_x_arr,axis = 0)*100\n",
    "\n",
    "\n",
    "fig = figure()\n",
    "frame = fig.add_axes([0,0,1,1])\n",
    "frame.plot(Pkcross_recon.k3D,difference,label='pre recon' ,color='r')\n",
    "frame.plot(Pkcross_recon.k3D,difference_recon,label='after recon' ,color='g')\n",
    "\n",
    "frame.set_xlim(0.02,0.4)\n",
    "frame.set_ylim(-5,200)\n",
    "\n",
    "frame.legend()\n",
    "# frame.set_title()\n",
    "\n",
    "frame.set_xlabel(\"$k$ [h/Mpc]\")\n",
    "formatter = FuncFormatter(to_percent)\n",
    "frame.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "frame.grid(which='major', linestyle='-', linewidth='0.6', color='gray')\n",
    "frame.grid(which='minor', linestyle=':', linewidth='0.2', color='gray') \n",
    "frame.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "frame.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "fig.savefig('Plots/correlation_diff.png',bbox_inches=\"tight\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26232878-f921-4922-ac28-1f7aa213564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = np.mean(cross_pre_arr,axis=0)\n",
    "pre_x = np.mean(cross_pre_x_arr,axis = 0)\n",
    "recon = np.mean(cross_recon_arr,axis=0)\n",
    "recon_x = np.mean(cross_recon_x_arr,axis = 0)\n",
    "\n",
    "fig = figure()\n",
    "frame = fig.add_axes([0,0,1,1])\n",
    "frame.plot(Pkcross_recon.k3D,pre,label='pre' ,color='cornflowerblue')\n",
    "frame.plot(Pkcross_recon.k3D,pre_x,label='pre RSD' ,color='orange')\n",
    "frame.plot(Pkcross_recon.k3D,recon,label='post' ,color='cornflowerblue')\n",
    "frame.plot(Pkcross_recon.k3D,recon_x,label='post RSD' ,color='orange')\n",
    "\n",
    "frame.set_xlim(0.02,0.4)\n",
    "frame.set_ylim(0,1)\n",
    "\n",
    "frame.legend()\n",
    "# frame.set_title()\n",
    "\n",
    "frame.set_xlabel(\"$k$ [h/Mpc]\")\n",
    "\n",
    "\n",
    "frame.grid(which='major', linestyle='-', linewidth='0.6', color='gray')\n",
    "frame.grid(which='minor', linestyle=':', linewidth='0.2', color='gray')\n",
    "frame.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "frame.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "fig.savefig('Plots/all_correlations.png',bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bff96-0f20-4092-af88-c15765105a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "\n",
    "# First subplot\n",
    "ax1.plot(Pkcross_recon.k3D, pre, label='pre', color='cornflowerblue')\n",
    "ax1.plot(Pkcross_recon.k3D, pre_x, label='pre RSD', color='orange')\n",
    "ax1.plot(Pkcross_recon.k3D, recon, label='post', color='cornflowerblue')\n",
    "ax1.plot(Pkcross_recon.k3D, recon_x, label='post RSD', color='orange')\n",
    "ax1.set_xlim(0.02, 0.4)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"$k$ [h/Mpc]\")\n",
    "ax1.set_ylabel(\"$C_{X,Y}(k)$\")\n",
    "ax1.grid(which='major', linestyle='-', linewidth='0.6', color='gray')\n",
    "ax1.grid(which='minor', linestyle=':', linewidth='0.25', color='gray')\n",
    "ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax1.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "# Second subplot\n",
    "ax2.plot(Pkcross_recon.k3D, difference, label='pre recon', color='r')\n",
    "ax2.plot(Pkcross_recon.k3D, difference_recon, label='after recon', color='g')\n",
    "ax2.set_xlim(0.02, 0.4)\n",
    "ax2.set_ylim(-5, 200)\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(\"$k$ [h/Mpc]\")\n",
    "formatter = FuncFormatter(to_percent)\n",
    "ax2.yaxis.set_major_formatter(formatter)\n",
    "ax2.grid(which='major', linestyle='-', linewidth='0.6', color='gray')\n",
    "ax2.grid(which='minor', linestyle=':', linewidth='0.25', color='gray')\n",
    "ax2.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax2.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "# Save the entire figure\n",
    "\n",
    "fig.savefig('Plots/combined_plots.png', bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8faee31-8ba0-4f91-bc09-2c8c7bbdd90a",
   "metadata": {},
   "source": [
    "From here the analysis can start using all the calculated spectra of the simulations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
